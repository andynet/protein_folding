{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "from optimize import optimize\n",
    "from structure import Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = []\n",
    "for l in [0.1, 0.05, 0.01, 0.005, 0.001]:\n",
    "    for m in [0, 0.1, 0.5, 0.9]:\n",
    "        params.append((l, m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 8204.844\n",
      "Iteration 020, Loss: 8496.800\n",
      "Iteration 030, Loss: 8495.096\n",
      "Iteration 040, Loss: 8086.151\n",
      "Iteration 050, Loss: 8264.975\n",
      "Iteration 060, Loss: 7997.896\n",
      "Iteration 070, Loss: 7701.389\n",
      "Iteration 080, Loss: 7558.087\n",
      "Iteration 090, Loss: 7659.876\n",
      "Iteration 100, Loss: 8198.678\n",
      "Iteration 110, Loss: 7971.159\n",
      "Iteration 120, Loss: 8073.090\n",
      "Iteration 130, Loss: 7777.972\n",
      "Iteration 140, Loss: 7699.140\n",
      "Iteration 150, Loss: 7575.258\n",
      "Iteration 160, Loss: 7788.343\n",
      "Iteration 170, Loss: 7482.069\n",
      "Iteration 180, Loss: 7793.444\n",
      "Iteration 190, Loss: 8322.661\n",
      "Iteration 199, Loss: 8599.385\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 8333.016\n",
      "Iteration 020, Loss: 8603.488\n",
      "Iteration 030, Loss: 8506.189\n",
      "Iteration 040, Loss: 7643.097\n",
      "Iteration 050, Loss: 8994.493\n",
      "Iteration 060, Loss: 8241.986\n",
      "Iteration 070, Loss: 8025.217\n",
      "Iteration 080, Loss: 7718.958\n",
      "Iteration 090, Loss: 7486.662\n",
      "Iteration 100, Loss: 8101.415\n",
      "Iteration 110, Loss: 8005.553\n",
      "Iteration 120, Loss: 8654.814\n",
      "Iteration 130, Loss: 7781.252\n",
      "Iteration 140, Loss: 7981.972\n",
      "Iteration 150, Loss: 7568.969\n",
      "Iteration 160, Loss: 7926.223\n",
      "Iteration 170, Loss: 7632.534\n",
      "Iteration 180, Loss: 7184.041\n",
      "Iteration 190, Loss: 7555.033\n",
      "Iteration 199, Loss: 8190.546\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 8042.591\n",
      "Iteration 020, Loss: 8641.951\n",
      "Iteration 030, Loss: 8613.688\n",
      "Iteration 040, Loss: 8586.991\n",
      "Iteration 050, Loss: 9029.524\n",
      "Iteration 060, Loss: 8751.357\n",
      "Iteration 070, Loss: 8814.928\n",
      "Iteration 080, Loss: 9552.502\n",
      "Iteration 090, Loss: 7664.448\n",
      "Iteration 100, Loss: 8134.848\n",
      "Iteration 110, Loss: 8262.669\n",
      "Iteration 120, Loss: 8155.174\n",
      "Iteration 130, Loss: 8183.470\n",
      "Iteration 140, Loss: 7957.172\n",
      "Iteration 150, Loss: 7972.104\n",
      "Iteration 160, Loss: 7626.132\n",
      "Iteration 170, Loss: 7307.638\n",
      "Iteration 180, Loss: 8485.313\n",
      "Iteration 190, Loss: 8069.114\n",
      "Iteration 199, Loss: 8326.686\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 10910.608\n",
      "Iteration 020, Loss: 8751.639\n",
      "Iteration 030, Loss: 9668.376\n",
      "Iteration 040, Loss: 11388.036\n",
      "Iteration 050, Loss: 10323.529\n",
      "Iteration 060, Loss: 10397.689\n",
      "Iteration 070, Loss: 10009.161\n",
      "Iteration 080, Loss: 11408.197\n",
      "Iteration 090, Loss: 9573.591\n",
      "Iteration 100, Loss: 9507.421\n",
      "Iteration 110, Loss: 9903.372\n",
      "Iteration 120, Loss: 9350.764\n",
      "Iteration 130, Loss: 8549.278\n",
      "Iteration 140, Loss: 9097.677\n",
      "Iteration 150, Loss: 9885.135\n",
      "Iteration 160, Loss: 9694.699\n",
      "Iteration 170, Loss: 9282.422\n",
      "Iteration 180, Loss: 10081.121\n",
      "Iteration 190, Loss: 10595.014\n",
      "Iteration 199, Loss: 9663.136\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 9347.201\n",
      "Iteration 020, Loss: 8467.639\n",
      "Iteration 030, Loss: 8394.244\n",
      "Iteration 040, Loss: 8503.603\n",
      "Iteration 050, Loss: 8303.997\n",
      "Iteration 060, Loss: 7845.385\n",
      "Iteration 070, Loss: 7951.251\n",
      "Iteration 080, Loss: 8141.154\n",
      "Iteration 090, Loss: 8178.862\n",
      "Iteration 100, Loss: 8145.673\n",
      "Iteration 110, Loss: 7375.074\n",
      "Iteration 120, Loss: 7505.533\n",
      "Iteration 130, Loss: 7526.525\n",
      "Iteration 140, Loss: 7276.731\n",
      "Iteration 150, Loss: 7178.499\n",
      "Iteration 160, Loss: 7216.226\n",
      "Iteration 170, Loss: 7406.683\n",
      "Iteration 180, Loss: 7104.308\n",
      "Iteration 190, Loss: 7109.758\n",
      "Iteration 199, Loss: 7152.938\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 9218.568\n",
      "Iteration 020, Loss: 8520.199\n",
      "Iteration 030, Loss: 8395.662\n",
      "Iteration 040, Loss: 8369.563\n",
      "Iteration 050, Loss: 7823.440\n",
      "Iteration 060, Loss: 7263.764\n",
      "Iteration 070, Loss: 7636.174\n",
      "Iteration 080, Loss: 7508.250\n",
      "Iteration 090, Loss: 7637.708\n",
      "Iteration 100, Loss: 7726.032\n",
      "Iteration 110, Loss: 7529.338\n",
      "Iteration 120, Loss: 7245.792\n",
      "Iteration 130, Loss: 7373.083\n",
      "Iteration 140, Loss: 7801.014\n",
      "Iteration 150, Loss: 7861.764\n",
      "Iteration 160, Loss: 7856.632\n",
      "Iteration 170, Loss: 8122.025\n",
      "Iteration 180, Loss: 8489.348\n",
      "Iteration 190, Loss: 8175.484\n",
      "Iteration 199, Loss: 8485.513\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 7970.989\n",
      "Iteration 020, Loss: 8928.273\n",
      "Iteration 030, Loss: 8093.446\n",
      "Iteration 040, Loss: 7930.889\n",
      "Iteration 050, Loss: 7729.956\n",
      "Iteration 060, Loss: 7849.688\n",
      "Iteration 070, Loss: 8221.628\n",
      "Iteration 080, Loss: 8165.808\n",
      "Iteration 090, Loss: 7763.363\n",
      "Iteration 100, Loss: 8279.387\n",
      "Iteration 110, Loss: 7648.656\n",
      "Iteration 120, Loss: 8366.285\n",
      "Iteration 130, Loss: 7997.590\n",
      "Iteration 140, Loss: 8431.062\n",
      "Iteration 150, Loss: 8531.556\n",
      "Iteration 160, Loss: 8276.753\n",
      "Iteration 170, Loss: 8620.747\n",
      "Iteration 180, Loss: 8362.953\n",
      "Iteration 190, Loss: 7275.727\n",
      "Iteration 199, Loss: 7398.923\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 9466.274\n",
      "Iteration 020, Loss: 9497.231\n",
      "Iteration 030, Loss: 9900.185\n",
      "Iteration 040, Loss: 10316.200\n",
      "Iteration 050, Loss: 9139.132\n",
      "Iteration 060, Loss: 9707.434\n",
      "Iteration 070, Loss: 8922.838\n",
      "Iteration 080, Loss: 9614.225\n",
      "Iteration 090, Loss: 9616.273\n",
      "Iteration 100, Loss: 10919.904\n",
      "Iteration 110, Loss: 11480.780\n",
      "Iteration 120, Loss: 9925.737\n",
      "Iteration 130, Loss: 9716.642\n",
      "Iteration 140, Loss: 9571.051\n",
      "Iteration 150, Loss: 10583.410\n",
      "Iteration 160, Loss: 9889.216\n",
      "Iteration 170, Loss: 11104.737\n",
      "Iteration 180, Loss: 10517.252\n",
      "Iteration 190, Loss: 9279.296\n",
      "Iteration 199, Loss: 10144.377\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 11059.913\n",
      "Iteration 020, Loss: 10592.908\n",
      "Iteration 030, Loss: 9826.861\n",
      "Iteration 040, Loss: 9547.249\n",
      "Iteration 050, Loss: 9320.945\n",
      "Iteration 060, Loss: 8920.051\n",
      "Iteration 070, Loss: 8357.954\n",
      "Iteration 080, Loss: 8277.599\n",
      "Iteration 090, Loss: 8409.905\n",
      "Iteration 100, Loss: 8477.464\n",
      "Iteration 110, Loss: 8479.394\n",
      "Iteration 120, Loss: 8432.771\n",
      "Iteration 130, Loss: 8368.488\n",
      "Iteration 140, Loss: 8326.357\n",
      "Iteration 150, Loss: 8358.259\n",
      "Iteration 160, Loss: 8409.510\n",
      "Iteration 170, Loss: 8462.408\n",
      "Iteration 180, Loss: 8462.265\n",
      "Iteration 190, Loss: 8411.688\n",
      "Iteration 199, Loss: 8315.165\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 11022.051\n",
      "Iteration 020, Loss: 10452.726\n",
      "Iteration 030, Loss: 9682.854\n",
      "Iteration 040, Loss: 9453.409\n",
      "Iteration 050, Loss: 9067.399\n",
      "Iteration 060, Loss: 8560.080\n",
      "Iteration 070, Loss: 8258.395\n",
      "Iteration 080, Loss: 8391.714\n",
      "Iteration 090, Loss: 8481.634\n",
      "Iteration 100, Loss: 8484.364\n",
      "Iteration 110, Loss: 8427.710\n",
      "Iteration 120, Loss: 8343.709\n",
      "Iteration 130, Loss: 8337.610\n",
      "Iteration 140, Loss: 8372.660\n",
      "Iteration 150, Loss: 8439.371\n",
      "Iteration 160, Loss: 8457.229\n",
      "Iteration 170, Loss: 8396.547\n",
      "Iteration 180, Loss: 8283.309\n",
      "Iteration 190, Loss: 8209.493\n",
      "Iteration 199, Loss: 8139.552\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 10708.814\n",
      "Iteration 020, Loss: 9574.864\n",
      "Iteration 030, Loss: 9044.623\n",
      "Iteration 040, Loss: 8237.011\n",
      "Iteration 050, Loss: 8449.844\n",
      "Iteration 060, Loss: 8470.691\n",
      "Iteration 070, Loss: 8360.867\n",
      "Iteration 080, Loss: 8428.003\n",
      "Iteration 090, Loss: 8568.700\n",
      "Iteration 100, Loss: 8458.492\n",
      "Iteration 110, Loss: 8274.114\n",
      "Iteration 120, Loss: 8210.139\n",
      "Iteration 130, Loss: 8008.350\n",
      "Iteration 140, Loss: 7709.637\n",
      "Iteration 150, Loss: 7437.156\n",
      "Iteration 160, Loss: 7309.764\n",
      "Iteration 170, Loss: 7459.709\n",
      "Iteration 180, Loss: 7603.117\n",
      "Iteration 190, Loss: 7673.590\n",
      "Iteration 199, Loss: 7628.575\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 9629.328\n",
      "Iteration 020, Loss: 8164.800\n",
      "Iteration 030, Loss: 9078.494\n",
      "Iteration 040, Loss: 9495.364\n",
      "Iteration 050, Loss: 9244.743\n",
      "Iteration 060, Loss: 8496.572\n",
      "Iteration 070, Loss: 8095.626\n",
      "Iteration 080, Loss: 8599.928\n",
      "Iteration 090, Loss: 8877.294\n",
      "Iteration 100, Loss: 9774.782\n",
      "Iteration 110, Loss: 8859.865\n",
      "Iteration 120, Loss: 8281.515\n",
      "Iteration 130, Loss: 7748.444\n",
      "Iteration 140, Loss: 8286.537\n",
      "Iteration 150, Loss: 7947.997\n",
      "Iteration 160, Loss: 8245.971\n",
      "Iteration 170, Loss: 8261.131\n",
      "Iteration 180, Loss: 8186.768\n",
      "Iteration 190, Loss: 7771.385\n",
      "Iteration 199, Loss: 7967.237\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 11227.566\n",
      "Iteration 020, Loss: 11058.427\n",
      "Iteration 030, Loss: 10840.545\n",
      "Iteration 040, Loss: 10588.836\n",
      "Iteration 050, Loss: 10191.618\n",
      "Iteration 060, Loss: 9830.438\n",
      "Iteration 070, Loss: 9642.415\n",
      "Iteration 080, Loss: 9547.438\n",
      "Iteration 090, Loss: 9445.018\n",
      "Iteration 100, Loss: 9327.551\n",
      "Iteration 110, Loss: 9087.715\n",
      "Iteration 120, Loss: 8930.254\n",
      "Iteration 130, Loss: 8654.331\n",
      "Iteration 140, Loss: 8378.202\n",
      "Iteration 150, Loss: 8269.495\n",
      "Iteration 160, Loss: 8288.399\n",
      "Iteration 170, Loss: 8351.670\n",
      "Iteration 180, Loss: 8415.959\n",
      "Iteration 190, Loss: 8446.265\n",
      "Iteration 199, Loss: 8479.217\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 11213.989\n",
      "Iteration 020, Loss: 11016.795\n",
      "Iteration 030, Loss: 10766.535\n",
      "Iteration 040, Loss: 10437.379\n",
      "Iteration 050, Loss: 9962.515\n",
      "Iteration 060, Loss: 9683.264\n",
      "Iteration 070, Loss: 9570.310\n",
      "Iteration 080, Loss: 9455.035\n",
      "Iteration 090, Loss: 9329.805\n",
      "Iteration 100, Loss: 9069.044\n",
      "Iteration 110, Loss: 8892.884\n",
      "Iteration 120, Loss: 8556.492\n",
      "Iteration 130, Loss: 8313.303\n",
      "Iteration 140, Loss: 8274.688\n",
      "Iteration 150, Loss: 8334.177\n",
      "Iteration 160, Loss: 8406.809\n",
      "Iteration 170, Loss: 8444.458\n",
      "Iteration 180, Loss: 8481.923\n",
      "Iteration 190, Loss: 8486.562\n",
      "Iteration 199, Loss: 8472.768\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 11099.317\n",
      "Iteration 020, Loss: 10653.427\n",
      "Iteration 030, Loss: 9881.312\n",
      "Iteration 040, Loss: 9553.620\n",
      "Iteration 050, Loss: 9352.018\n",
      "Iteration 060, Loss: 8959.319\n",
      "Iteration 070, Loss: 8428.712\n",
      "Iteration 080, Loss: 8252.182\n",
      "Iteration 090, Loss: 8387.423\n",
      "Iteration 100, Loss: 8481.740\n",
      "Iteration 110, Loss: 8516.260\n",
      "Iteration 120, Loss: 8460.850\n",
      "Iteration 130, Loss: 8382.741\n",
      "Iteration 140, Loss: 8333.613\n",
      "Iteration 150, Loss: 8351.381\n",
      "Iteration 160, Loss: 8408.271\n",
      "Iteration 170, Loss: 8482.022\n",
      "Iteration 180, Loss: 8514.253\n",
      "Iteration 190, Loss: 8457.175\n",
      "Iteration 199, Loss: 8351.087\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 10601.530\n",
      "Iteration 020, Loss: 9058.140\n",
      "Iteration 030, Loss: 8121.181\n",
      "Iteration 040, Loss: 8797.109\n",
      "Iteration 050, Loss: 9195.256\n",
      "Iteration 060, Loss: 9321.237\n",
      "Iteration 070, Loss: 9007.601\n",
      "Iteration 080, Loss: 8959.995\n",
      "Iteration 090, Loss: 8748.720\n",
      "Iteration 100, Loss: 8543.968\n",
      "Iteration 110, Loss: 8157.127\n",
      "Iteration 120, Loss: 8165.456\n",
      "Iteration 130, Loss: 8451.755\n",
      "Iteration 140, Loss: 8544.424\n",
      "Iteration 150, Loss: 8968.972\n",
      "Iteration 160, Loss: 8707.255\n",
      "Iteration 170, Loss: 8479.027\n",
      "Iteration 180, Loss: 8128.445\n",
      "Iteration 190, Loss: 8297.945\n",
      "Iteration 199, Loss: 8527.277\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 11332.204\n",
      "Iteration 020, Loss: 11307.699\n",
      "Iteration 030, Loss: 11282.076\n",
      "Iteration 040, Loss: 11254.932\n",
      "Iteration 050, Loss: 11227.320\n",
      "Iteration 060, Loss: 11199.048\n",
      "Iteration 070, Loss: 11166.271\n",
      "Iteration 080, Loss: 11131.896\n",
      "Iteration 090, Loss: 11096.080\n",
      "Iteration 100, Loss: 11057.690\n",
      "Iteration 110, Loss: 11018.886\n",
      "Iteration 120, Loss: 10978.265\n",
      "Iteration 130, Loss: 10931.885\n",
      "Iteration 140, Loss: 10883.637\n",
      "Iteration 150, Loss: 10839.560\n",
      "Iteration 160, Loss: 10794.568\n",
      "Iteration 170, Loss: 10746.390\n",
      "Iteration 180, Loss: 10698.301\n",
      "Iteration 190, Loss: 10645.944\n",
      "Iteration 199, Loss: 10593.554\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 11329.814\n",
      "Iteration 020, Loss: 11302.543\n",
      "Iteration 030, Loss: 11273.376\n",
      "Iteration 040, Loss: 11243.002\n",
      "Iteration 050, Loss: 11212.162\n",
      "Iteration 060, Loss: 11178.084\n",
      "Iteration 070, Loss: 11139.693\n",
      "Iteration 080, Loss: 11100.607\n",
      "Iteration 090, Loss: 11058.049\n",
      "Iteration 100, Loss: 11014.924\n",
      "Iteration 110, Loss: 10969.429\n",
      "Iteration 120, Loss: 10915.742\n",
      "Iteration 130, Loss: 10864.196\n",
      "Iteration 140, Loss: 10815.371\n",
      "Iteration 150, Loss: 10763.272\n",
      "Iteration 160, Loss: 10710.097\n",
      "Iteration 170, Loss: 10652.378\n",
      "Iteration 180, Loss: 10587.872\n",
      "Iteration 190, Loss: 10515.038\n",
      "Iteration 199, Loss: 10435.271\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 11312.710\n",
      "Iteration 020, Loss: 11260.964\n",
      "Iteration 030, Loss: 11205.482\n",
      "Iteration 040, Loss: 11139.255\n",
      "Iteration 050, Loss: 11066.084\n",
      "Iteration 060, Loss: 10988.172\n",
      "Iteration 070, Loss: 10895.075\n",
      "Iteration 080, Loss: 10805.433\n",
      "Iteration 090, Loss: 10710.580\n",
      "Iteration 100, Loss: 10603.094\n",
      "Iteration 110, Loss: 10466.180\n",
      "Iteration 120, Loss: 10299.564\n",
      "Iteration 130, Loss: 10121.405\n",
      "Iteration 140, Loss: 9956.334\n",
      "Iteration 150, Loss: 9838.504\n",
      "Iteration 160, Loss: 9736.688\n",
      "Iteration 170, Loss: 9669.528\n",
      "Iteration 180, Loss: 9626.608\n",
      "Iteration 190, Loss: 9593.432\n",
      "Iteration 199, Loss: 9555.715\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 11253.210\n",
      "Iteration 020, Loss: 10990.260\n",
      "Iteration 030, Loss: 10539.040\n",
      "Iteration 040, Loss: 9798.527\n",
      "Iteration 050, Loss: 9519.756\n",
      "Iteration 060, Loss: 9290.473\n",
      "Iteration 070, Loss: 9001.950\n",
      "Iteration 080, Loss: 8521.681\n",
      "Iteration 090, Loss: 8147.457\n",
      "Iteration 100, Loss: 8231.426\n",
      "Iteration 110, Loss: 8481.659\n",
      "Iteration 120, Loss: 8635.669\n",
      "Iteration 130, Loss: 8764.991\n",
      "Iteration 140, Loss: 8870.251\n",
      "Iteration 150, Loss: 8869.186\n",
      "Iteration 160, Loss: 8846.739\n",
      "Iteration 170, Loss: 8746.056\n",
      "Iteration 180, Loss: 8628.155\n",
      "Iteration 190, Loss: 8626.831\n",
      "Iteration 199, Loss: 8710.174\n"
     ]
    }
   ],
   "source": [
    "optimized2 = {}\n",
    "for p in params:\n",
    "    s = optimize('1z6mA02', random_state=1, iterations=200, lr=p[0], \n",
    "                 momentum=p[1], normal=True, angle_potential=False, normalize_gradients=False,\n",
    "                scale_gradients=True)\n",
    "    optimized2[f'{p}'] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('1z6mA02_trials/1z6m_smooth_scaled_21.pkl', 'wb') as f:\n",
    "    pickle.dump(optimized2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 14364.083\n",
      "Iteration 020, Loss: 10385.405\n",
      "Iteration 030, Loss: 7481.688\n",
      "Iteration 040, Loss: 7143.552\n",
      "Iteration 050, Loss: 7139.437\n",
      "Iteration 060, Loss: 7390.847\n",
      "Iteration 070, Loss: 7892.890\n",
      "Iteration 080, Loss: 8169.842\n",
      "Iteration 090, Loss: 10004.149\n",
      "Iteration 100, Loss: 8828.519\n",
      "Iteration 110, Loss: 8186.657\n",
      "Iteration 120, Loss: 8291.311\n",
      "Iteration 130, Loss: 8438.497\n",
      "Iteration 140, Loss: 8242.597\n",
      "Iteration 150, Loss: 7733.881\n",
      "Iteration 160, Loss: 7835.047\n",
      "Iteration 170, Loss: 7848.497\n",
      "Iteration 180, Loss: 8450.081\n",
      "Iteration 190, Loss: 8522.543\n",
      "Iteration 199, Loss: 8680.012\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 14025.703\n",
      "Iteration 020, Loss: 10099.482\n",
      "Iteration 030, Loss: 7490.286\n",
      "Iteration 040, Loss: 7070.770\n",
      "Iteration 050, Loss: 7384.529\n",
      "Iteration 060, Loss: 7326.447\n",
      "Iteration 070, Loss: 7392.233\n",
      "Iteration 080, Loss: 7372.225\n",
      "Iteration 090, Loss: 7165.353\n",
      "Iteration 100, Loss: 8385.681\n",
      "Iteration 110, Loss: 8365.952\n",
      "Iteration 120, Loss: 8769.535\n",
      "Iteration 130, Loss: 8531.408\n",
      "Iteration 140, Loss: 8092.552\n",
      "Iteration 150, Loss: 7681.308\n",
      "Iteration 160, Loss: 7826.314\n",
      "Iteration 170, Loss: 7716.631\n",
      "Iteration 180, Loss: 8983.240\n",
      "Iteration 190, Loss: 8865.122\n",
      "Iteration 199, Loss: 9388.550\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 11522.435\n",
      "Iteration 020, Loss: 7878.008\n",
      "Iteration 030, Loss: 7726.917\n",
      "Iteration 040, Loss: 7930.954\n",
      "Iteration 050, Loss: 8320.704\n",
      "Iteration 060, Loss: 7602.785\n",
      "Iteration 070, Loss: 7716.542\n",
      "Iteration 080, Loss: 8316.162\n",
      "Iteration 090, Loss: 8614.078\n",
      "Iteration 100, Loss: 8336.669\n",
      "Iteration 110, Loss: 8720.498\n",
      "Iteration 120, Loss: 8231.435\n",
      "Iteration 130, Loss: 8520.264\n",
      "Iteration 140, Loss: 7974.967\n",
      "Iteration 150, Loss: 7993.581\n",
      "Iteration 160, Loss: 7970.463\n",
      "Iteration 170, Loss: 8485.604\n",
      "Iteration 180, Loss: 8247.547\n",
      "Iteration 190, Loss: 8161.300\n",
      "Iteration 199, Loss: 7750.563\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 9486.857\n",
      "Iteration 020, Loss: 11443.061\n",
      "Iteration 030, Loss: 9754.814\n",
      "Iteration 040, Loss: 10037.283\n",
      "Iteration 050, Loss: 10128.003\n",
      "Iteration 060, Loss: 10070.226\n",
      "Iteration 070, Loss: 10212.709\n",
      "Iteration 080, Loss: 11106.081\n",
      "Iteration 090, Loss: 12356.715\n",
      "Iteration 100, Loss: 9383.872\n",
      "Iteration 110, Loss: 9233.035\n",
      "Iteration 120, Loss: 9690.840\n",
      "Iteration 130, Loss: 10931.267\n",
      "Iteration 140, Loss: 8980.091\n",
      "Iteration 150, Loss: 10720.562\n",
      "Iteration 160, Loss: 11116.569\n",
      "Iteration 170, Loss: 8987.042\n",
      "Iteration 180, Loss: 8836.763\n",
      "Iteration 190, Loss: 9635.511\n",
      "Iteration 199, Loss: 9601.030\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 15189.781\n",
      "Iteration 020, Loss: 14368.588\n",
      "Iteration 030, Loss: 12646.288\n",
      "Iteration 040, Loss: 10419.042\n",
      "Iteration 050, Loss: 7998.003\n",
      "Iteration 060, Loss: 7424.291\n",
      "Iteration 070, Loss: 7149.492\n",
      "Iteration 080, Loss: 7188.437\n",
      "Iteration 090, Loss: 7060.728\n",
      "Iteration 100, Loss: 7136.438\n",
      "Iteration 110, Loss: 6948.544\n",
      "Iteration 120, Loss: 6275.529\n",
      "Iteration 130, Loss: 7171.166\n",
      "Iteration 140, Loss: 7827.760\n",
      "Iteration 150, Loss: 8032.036\n",
      "Iteration 160, Loss: 8135.899\n",
      "Iteration 170, Loss: 8353.982\n",
      "Iteration 180, Loss: 8277.179\n",
      "Iteration 190, Loss: 8077.549\n",
      "Iteration 199, Loss: 8022.867\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 15144.962\n",
      "Iteration 020, Loss: 14086.474\n",
      "Iteration 030, Loss: 12076.643\n",
      "Iteration 040, Loss: 10186.364\n",
      "Iteration 050, Loss: 7270.493\n",
      "Iteration 060, Loss: 7140.075\n",
      "Iteration 070, Loss: 7012.625\n",
      "Iteration 080, Loss: 7066.973\n",
      "Iteration 090, Loss: 7136.248\n",
      "Iteration 100, Loss: 7281.074\n",
      "Iteration 110, Loss: 7493.476\n",
      "Iteration 120, Loss: 7801.429\n",
      "Iteration 130, Loss: 7436.625\n",
      "Iteration 140, Loss: 7862.758\n",
      "Iteration 150, Loss: 8602.126\n",
      "Iteration 160, Loss: 8133.707\n",
      "Iteration 170, Loss: 8645.210\n",
      "Iteration 180, Loss: 8420.107\n",
      "Iteration 190, Loss: 8068.763\n",
      "Iteration 199, Loss: 7406.014\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 14570.329\n",
      "Iteration 020, Loss: 10599.215\n",
      "Iteration 030, Loss: 7010.568\n",
      "Iteration 040, Loss: 7456.505\n",
      "Iteration 050, Loss: 7600.900\n",
      "Iteration 060, Loss: 7483.177\n",
      "Iteration 070, Loss: 7197.906\n",
      "Iteration 080, Loss: 7297.485\n",
      "Iteration 090, Loss: 7113.051\n",
      "Iteration 100, Loss: 10163.235\n",
      "Iteration 110, Loss: 9162.028\n",
      "Iteration 120, Loss: 8714.235\n",
      "Iteration 130, Loss: 8732.715\n",
      "Iteration 140, Loss: 8110.055\n",
      "Iteration 150, Loss: 7738.687\n",
      "Iteration 160, Loss: 8199.771\n",
      "Iteration 170, Loss: 7805.049\n",
      "Iteration 180, Loss: 8042.630\n",
      "Iteration 190, Loss: 7854.716\n",
      "Iteration 199, Loss: 7613.409\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 11485.990\n",
      "Iteration 020, Loss: 9501.255\n",
      "Iteration 030, Loss: 9317.153\n",
      "Iteration 040, Loss: 9767.088\n",
      "Iteration 050, Loss: 9069.920\n",
      "Iteration 060, Loss: 10906.765\n",
      "Iteration 070, Loss: 9713.671\n",
      "Iteration 080, Loss: 9280.082\n",
      "Iteration 090, Loss: 10412.573\n",
      "Iteration 100, Loss: 10057.733\n",
      "Iteration 110, Loss: 9107.239\n",
      "Iteration 120, Loss: 8858.620\n",
      "Iteration 130, Loss: 10253.655\n",
      "Iteration 140, Loss: 10577.909\n",
      "Iteration 150, Loss: 10340.901\n",
      "Iteration 160, Loss: 9041.889\n",
      "Iteration 170, Loss: 9048.087\n",
      "Iteration 180, Loss: 8800.762\n",
      "Iteration 190, Loss: 8967.403\n",
      "Iteration 199, Loss: 8650.854\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 15473.521\n",
      "Iteration 020, Loss: 15423.137\n",
      "Iteration 030, Loss: 15363.315\n",
      "Iteration 040, Loss: 15283.583\n",
      "Iteration 050, Loss: 15189.945\n",
      "Iteration 060, Loss: 15091.905\n",
      "Iteration 070, Loss: 14960.562\n",
      "Iteration 080, Loss: 14792.760\n",
      "Iteration 090, Loss: 14598.762\n",
      "Iteration 100, Loss: 14364.290\n",
      "Iteration 110, Loss: 14094.074\n",
      "Iteration 120, Loss: 13813.272\n",
      "Iteration 130, Loss: 13527.111\n",
      "Iteration 140, Loss: 13061.420\n",
      "Iteration 150, Loss: 12628.466\n",
      "Iteration 160, Loss: 12295.309\n",
      "Iteration 170, Loss: 11861.879\n",
      "Iteration 180, Loss: 11259.345\n",
      "Iteration 190, Loss: 10707.480\n",
      "Iteration 199, Loss: 10470.467\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 15468.383\n",
      "Iteration 020, Loss: 15411.414\n",
      "Iteration 030, Loss: 15339.678\n",
      "Iteration 040, Loss: 15243.120\n",
      "Iteration 050, Loss: 15138.043\n",
      "Iteration 060, Loss: 15010.926\n",
      "Iteration 070, Loss: 14837.976\n",
      "Iteration 080, Loss: 14625.889\n",
      "Iteration 090, Loss: 14369.536\n",
      "Iteration 100, Loss: 14062.473\n",
      "Iteration 110, Loss: 13751.485\n",
      "Iteration 120, Loss: 13406.172\n",
      "Iteration 130, Loss: 12844.049\n",
      "Iteration 140, Loss: 12445.944\n",
      "Iteration 150, Loss: 12003.695\n",
      "Iteration 160, Loss: 11416.357\n",
      "Iteration 170, Loss: 10739.820\n",
      "Iteration 180, Loss: 10448.123\n",
      "Iteration 190, Loss: 10351.949\n",
      "Iteration 199, Loss: 10134.394\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 15432.199\n",
      "Iteration 020, Loss: 15302.919\n",
      "Iteration 030, Loss: 15110.854\n",
      "Iteration 040, Loss: 14827.600\n",
      "Iteration 050, Loss: 14413.816\n",
      "Iteration 060, Loss: 13837.968\n",
      "Iteration 070, Loss: 13168.861\n",
      "Iteration 080, Loss: 12343.570\n",
      "Iteration 090, Loss: 11368.536\n",
      "Iteration 100, Loss: 10393.052\n",
      "Iteration 110, Loss: 10233.213\n",
      "Iteration 120, Loss: 9505.842\n",
      "Iteration 130, Loss: 7240.391\n",
      "Iteration 140, Loss: 7314.893\n",
      "Iteration 150, Loss: 7411.085\n",
      "Iteration 160, Loss: 7230.138\n",
      "Iteration 170, Loss: 7135.639\n",
      "Iteration 180, Loss: 7090.403\n",
      "Iteration 190, Loss: 7059.755\n",
      "Iteration 199, Loss: 6990.758\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 15273.952\n",
      "Iteration 020, Loss: 13862.527\n",
      "Iteration 030, Loss: 11029.333\n",
      "Iteration 040, Loss: 9117.432\n",
      "Iteration 050, Loss: 8539.072\n",
      "Iteration 060, Loss: 8130.838\n",
      "Iteration 070, Loss: 7936.889\n",
      "Iteration 080, Loss: 7757.559\n",
      "Iteration 090, Loss: 7971.235\n",
      "Iteration 100, Loss: 8427.521\n",
      "Iteration 110, Loss: 9070.259\n",
      "Iteration 120, Loss: 9091.074\n",
      "Iteration 130, Loss: 9094.168\n",
      "Iteration 140, Loss: 9520.276\n",
      "Iteration 150, Loss: 9332.254\n",
      "Iteration 160, Loss: 9150.539\n",
      "Iteration 170, Loss: 8834.912\n",
      "Iteration 180, Loss: 8965.205\n",
      "Iteration 190, Loss: 8573.721\n",
      "Iteration 199, Loss: 9158.441\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 15497.938\n",
      "Iteration 020, Loss: 15473.637\n",
      "Iteration 030, Loss: 15449.329\n",
      "Iteration 040, Loss: 15423.461\n",
      "Iteration 050, Loss: 15395.828\n",
      "Iteration 060, Loss: 15363.439\n",
      "Iteration 070, Loss: 15326.304\n",
      "Iteration 080, Loss: 15283.508\n",
      "Iteration 090, Loss: 15236.850\n",
      "Iteration 100, Loss: 15190.242\n",
      "Iteration 110, Loss: 15141.551\n",
      "Iteration 120, Loss: 15092.516\n",
      "Iteration 130, Loss: 15030.245\n",
      "Iteration 140, Loss: 14962.226\n",
      "Iteration 150, Loss: 14885.870\n",
      "Iteration 160, Loss: 14795.654\n",
      "Iteration 170, Loss: 14701.939\n",
      "Iteration 180, Loss: 14602.037\n",
      "Iteration 190, Loss: 14492.388\n",
      "Iteration 199, Loss: 14381.089\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 15495.534\n",
      "Iteration 020, Loss: 15468.515\n",
      "Iteration 030, Loss: 15441.101\n",
      "Iteration 040, Loss: 15411.657\n",
      "Iteration 050, Loss: 15379.010\n",
      "Iteration 060, Loss: 15339.554\n",
      "Iteration 070, Loss: 15293.917\n",
      "Iteration 080, Loss: 15242.669\n",
      "Iteration 090, Loss: 15191.076\n",
      "Iteration 100, Loss: 15137.583\n",
      "Iteration 110, Loss: 15081.140\n",
      "Iteration 120, Loss: 15009.167\n",
      "Iteration 130, Loss: 14930.715\n",
      "Iteration 140, Loss: 14839.237\n",
      "Iteration 150, Loss: 14733.998\n",
      "Iteration 160, Loss: 14627.572\n",
      "Iteration 170, Loss: 14506.956\n",
      "Iteration 180, Loss: 14369.387\n",
      "Iteration 190, Loss: 14220.548\n",
      "Iteration 199, Loss: 14083.990\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 15478.251\n",
      "Iteration 020, Loss: 15427.723\n",
      "Iteration 030, Loss: 15369.690\n",
      "Iteration 040, Loss: 15293.196\n",
      "Iteration 050, Loss: 15200.358\n",
      "Iteration 060, Loss: 15101.985\n",
      "Iteration 070, Loss: 14974.321\n",
      "Iteration 080, Loss: 14808.378\n",
      "Iteration 090, Loss: 14614.493\n",
      "Iteration 100, Loss: 14388.823\n",
      "Iteration 110, Loss: 14109.173\n",
      "Iteration 120, Loss: 13822.872\n",
      "Iteration 130, Loss: 13546.562\n",
      "Iteration 140, Loss: 13108.610\n",
      "Iteration 150, Loss: 12659.151\n",
      "Iteration 160, Loss: 12314.097\n",
      "Iteration 170, Loss: 11880.789\n",
      "Iteration 180, Loss: 11301.899\n",
      "Iteration 190, Loss: 10700.183\n",
      "Iteration 199, Loss: 10448.622\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 15414.316\n",
      "Iteration 020, Loss: 15095.534\n",
      "Iteration 030, Loss: 14193.860\n",
      "Iteration 040, Loss: 12438.055\n",
      "Iteration 050, Loss: 10724.671\n",
      "Iteration 060, Loss: 9440.297\n",
      "Iteration 070, Loss: 8292.423\n",
      "Iteration 080, Loss: 7004.953\n",
      "Iteration 090, Loss: 6917.213\n",
      "Iteration 100, Loss: 6638.420\n",
      "Iteration 110, Loss: 6387.515\n",
      "Iteration 120, Loss: 6780.421\n",
      "Iteration 130, Loss: 7354.475\n",
      "Iteration 140, Loss: 6855.086\n",
      "Iteration 150, Loss: 7342.838\n",
      "Iteration 160, Loss: 7152.941\n",
      "Iteration 170, Loss: 6951.424\n",
      "Iteration 180, Loss: 8469.655\n",
      "Iteration 190, Loss: 8844.426\n",
      "Iteration 199, Loss: 8717.074\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 15516.463\n",
      "Iteration 020, Loss: 15512.216\n",
      "Iteration 030, Loss: 15507.541\n",
      "Iteration 040, Loss: 15502.718\n",
      "Iteration 050, Loss: 15497.929\n",
      "Iteration 060, Loss: 15493.107\n",
      "Iteration 070, Loss: 15488.275\n",
      "Iteration 080, Loss: 15483.398\n",
      "Iteration 090, Loss: 15478.560\n",
      "Iteration 100, Loss: 15473.681\n",
      "Iteration 110, Loss: 15468.927\n",
      "Iteration 120, Loss: 15464.242\n",
      "Iteration 130, Loss: 15459.418\n",
      "Iteration 140, Loss: 15454.484\n",
      "Iteration 150, Loss: 15449.442\n",
      "Iteration 160, Loss: 15444.354\n",
      "Iteration 170, Loss: 15439.271\n",
      "Iteration 180, Loss: 15434.180\n",
      "Iteration 190, Loss: 15428.914\n",
      "Iteration 199, Loss: 15424.089\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 15516.050\n",
      "Iteration 020, Loss: 15511.223\n",
      "Iteration 030, Loss: 15506.001\n",
      "Iteration 040, Loss: 15500.643\n",
      "Iteration 050, Loss: 15495.314\n",
      "Iteration 060, Loss: 15489.937\n",
      "Iteration 070, Loss: 15484.536\n",
      "Iteration 080, Loss: 15479.124\n",
      "Iteration 090, Loss: 15473.733\n",
      "Iteration 100, Loss: 15468.429\n",
      "Iteration 110, Loss: 15463.193\n",
      "Iteration 120, Loss: 15457.827\n",
      "Iteration 130, Loss: 15452.284\n",
      "Iteration 140, Loss: 15446.655\n",
      "Iteration 150, Loss: 15440.989\n",
      "Iteration 160, Loss: 15435.331\n",
      "Iteration 170, Loss: 15429.544\n",
      "Iteration 180, Loss: 15423.599\n",
      "Iteration 190, Loss: 15417.647\n",
      "Iteration 199, Loss: 15412.168\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 15513.078\n",
      "Iteration 020, Loss: 15503.671\n",
      "Iteration 030, Loss: 15494.051\n",
      "Iteration 040, Loss: 15484.336\n",
      "Iteration 050, Loss: 15474.582\n",
      "Iteration 060, Loss: 15465.049\n",
      "Iteration 070, Loss: 15455.322\n",
      "Iteration 080, Loss: 15445.221\n",
      "Iteration 090, Loss: 15435.016\n",
      "Iteration 100, Loss: 15424.419\n",
      "Iteration 110, Loss: 15413.646\n",
      "Iteration 120, Loss: 15402.555\n",
      "Iteration 130, Loss: 15390.957\n",
      "Iteration 140, Loss: 15378.471\n",
      "Iteration 150, Loss: 15364.793\n",
      "Iteration 160, Loss: 15350.325\n",
      "Iteration 170, Loss: 15335.522\n",
      "Iteration 180, Loss: 15319.956\n",
      "Iteration 190, Loss: 15302.899\n",
      "Iteration 199, Loss: 15287.097\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 15502.098\n",
      "Iteration 020, Loss: 15462.532\n",
      "Iteration 030, Loss: 15411.980\n",
      "Iteration 040, Loss: 15351.928\n",
      "Iteration 050, Loss: 15278.544\n",
      "Iteration 060, Loss: 15187.900\n",
      "Iteration 070, Loss: 15080.394\n",
      "Iteration 080, Loss: 14947.335\n",
      "Iteration 090, Loss: 14762.558\n",
      "Iteration 100, Loss: 14552.080\n",
      "Iteration 110, Loss: 14308.759\n",
      "Iteration 120, Loss: 14016.479\n",
      "Iteration 130, Loss: 13704.723\n",
      "Iteration 140, Loss: 13406.357\n",
      "Iteration 150, Loss: 12913.051\n",
      "Iteration 160, Loss: 12543.445\n",
      "Iteration 170, Loss: 12172.634\n",
      "Iteration 180, Loss: 11731.747\n",
      "Iteration 190, Loss: 11114.772\n",
      "Iteration 199, Loss: 10562.199\n"
     ]
    }
   ],
   "source": [
    "optimized22 = {}\n",
    "for p in params:\n",
    "    s = optimize('1z6mA02', random_state=2, iterations=200, lr=p[0], \n",
    "                 momentum=p[1], normal=True, angle_potential=False, normalize_gradients=False,\n",
    "                scale_gradients=True)\n",
    "    optimized22[f'{p}'] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('1z6mA02_trials/1z6m_smooth_scaled_22.pkl', 'wb') as f:\n",
    "    pickle.dump(optimized22, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 8058.045\n",
      "Iteration 020, Loss: 9390.946\n",
      "Iteration 030, Loss: 8773.953\n",
      "Iteration 040, Loss: 8152.431\n",
      "Iteration 050, Loss: 8668.108\n",
      "Iteration 060, Loss: 8849.158\n",
      "Iteration 070, Loss: 8533.037\n",
      "Iteration 080, Loss: 8381.971\n",
      "Iteration 090, Loss: 8201.750\n",
      "Iteration 100, Loss: 8248.263\n",
      "Iteration 110, Loss: 8285.494\n",
      "Iteration 120, Loss: 8438.127\n",
      "Iteration 130, Loss: 8652.905\n",
      "Iteration 140, Loss: 8993.841\n",
      "Iteration 150, Loss: 8888.389\n",
      "Iteration 160, Loss: 8485.093\n",
      "Iteration 170, Loss: 8243.567\n",
      "Iteration 180, Loss: 8527.729\n",
      "Iteration 190, Loss: 8156.979\n",
      "Iteration 199, Loss: 7933.199\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 7823.404\n",
      "Iteration 020, Loss: 9569.656\n",
      "Iteration 030, Loss: 8969.290\n",
      "Iteration 040, Loss: 8745.844\n",
      "Iteration 050, Loss: 7690.148\n",
      "Iteration 060, Loss: 8059.301\n",
      "Iteration 070, Loss: 7820.664\n",
      "Iteration 080, Loss: 8118.896\n",
      "Iteration 090, Loss: 7687.772\n",
      "Iteration 100, Loss: 7910.640\n",
      "Iteration 110, Loss: 7583.051\n",
      "Iteration 120, Loss: 7668.390\n",
      "Iteration 130, Loss: 7556.512\n",
      "Iteration 140, Loss: 7471.591\n",
      "Iteration 150, Loss: 7585.868\n",
      "Iteration 160, Loss: 7821.563\n",
      "Iteration 170, Loss: 7773.070\n",
      "Iteration 180, Loss: 7915.896\n",
      "Iteration 190, Loss: 7651.911\n",
      "Iteration 199, Loss: 7575.218\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 8378.050\n",
      "Iteration 020, Loss: 8159.188\n",
      "Iteration 030, Loss: 7157.346\n",
      "Iteration 040, Loss: 8093.331\n",
      "Iteration 050, Loss: 7927.170\n",
      "Iteration 060, Loss: 8294.698\n",
      "Iteration 070, Loss: 8772.336\n",
      "Iteration 080, Loss: 8369.465\n",
      "Iteration 090, Loss: 8253.597\n",
      "Iteration 100, Loss: 8271.466\n",
      "Iteration 110, Loss: 7841.658\n",
      "Iteration 120, Loss: 9565.489\n",
      "Iteration 130, Loss: 8337.118\n",
      "Iteration 140, Loss: 8421.986\n",
      "Iteration 150, Loss: 8394.364\n",
      "Iteration 160, Loss: 7852.197\n",
      "Iteration 170, Loss: 7990.012\n",
      "Iteration 180, Loss: 7952.147\n",
      "Iteration 190, Loss: 8618.451\n",
      "Iteration 199, Loss: 9273.095\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 9628.353\n",
      "Iteration 020, Loss: 9966.961\n",
      "Iteration 030, Loss: 11122.926\n",
      "Iteration 040, Loss: 9398.748\n",
      "Iteration 050, Loss: 11646.883\n",
      "Iteration 060, Loss: 9195.271\n",
      "Iteration 070, Loss: 10146.563\n",
      "Iteration 080, Loss: 9574.380\n",
      "Iteration 090, Loss: 9810.248\n",
      "Iteration 100, Loss: 10840.719\n",
      "Iteration 110, Loss: 9800.495\n",
      "Iteration 120, Loss: 9632.272\n",
      "Iteration 130, Loss: 11351.052\n",
      "Iteration 140, Loss: 9849.261\n",
      "Iteration 150, Loss: 10456.303\n",
      "Iteration 160, Loss: 9793.834\n",
      "Iteration 170, Loss: 8954.294\n",
      "Iteration 180, Loss: 10935.413\n",
      "Iteration 190, Loss: 10334.061\n",
      "Iteration 199, Loss: 9985.058\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 8295.097\n",
      "Iteration 020, Loss: 7551.911\n",
      "Iteration 030, Loss: 9067.815\n",
      "Iteration 040, Loss: 8144.615\n",
      "Iteration 050, Loss: 8027.643\n",
      "Iteration 060, Loss: 7917.279\n",
      "Iteration 070, Loss: 7851.194\n",
      "Iteration 080, Loss: 7760.673\n",
      "Iteration 090, Loss: 7741.029\n",
      "Iteration 100, Loss: 7626.826\n",
      "Iteration 110, Loss: 8318.663\n",
      "Iteration 120, Loss: 7993.717\n",
      "Iteration 130, Loss: 7819.043\n",
      "Iteration 140, Loss: 7641.268\n",
      "Iteration 150, Loss: 8185.682\n",
      "Iteration 160, Loss: 8049.017\n",
      "Iteration 170, Loss: 8163.139\n",
      "Iteration 180, Loss: 8017.577\n",
      "Iteration 190, Loss: 8245.641\n",
      "Iteration 199, Loss: 8864.665\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 8387.721\n",
      "Iteration 020, Loss: 7527.607\n",
      "Iteration 030, Loss: 8940.794\n",
      "Iteration 040, Loss: 8122.298\n",
      "Iteration 050, Loss: 7986.142\n",
      "Iteration 060, Loss: 7852.366\n",
      "Iteration 070, Loss: 7797.178\n",
      "Iteration 080, Loss: 7415.797\n",
      "Iteration 090, Loss: 8618.168\n",
      "Iteration 100, Loss: 8153.737\n",
      "Iteration 110, Loss: 10173.646\n",
      "Iteration 120, Loss: 10083.986\n",
      "Iteration 130, Loss: 9256.941\n",
      "Iteration 140, Loss: 8636.628\n",
      "Iteration 150, Loss: 8511.631\n",
      "Iteration 160, Loss: 8348.309\n",
      "Iteration 170, Loss: 8146.050\n",
      "Iteration 180, Loss: 8564.062\n",
      "Iteration 190, Loss: 7917.027\n",
      "Iteration 199, Loss: 7793.326\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 7961.004\n",
      "Iteration 020, Loss: 8721.413\n",
      "Iteration 030, Loss: 8470.726\n",
      "Iteration 040, Loss: 7636.750\n",
      "Iteration 050, Loss: 7797.154\n",
      "Iteration 060, Loss: 7734.341\n",
      "Iteration 070, Loss: 7050.257\n",
      "Iteration 080, Loss: 7313.180\n",
      "Iteration 090, Loss: 7633.748\n",
      "Iteration 100, Loss: 7630.931\n",
      "Iteration 110, Loss: 7201.578\n",
      "Iteration 120, Loss: 7701.893\n",
      "Iteration 130, Loss: 8251.058\n",
      "Iteration 140, Loss: 8697.252\n",
      "Iteration 150, Loss: 8210.657\n",
      "Iteration 160, Loss: 8190.314\n",
      "Iteration 170, Loss: 7906.964\n",
      "Iteration 180, Loss: 7866.995\n",
      "Iteration 190, Loss: 7117.744\n",
      "Iteration 199, Loss: 7268.622\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 9365.489\n",
      "Iteration 020, Loss: 10427.097\n",
      "Iteration 030, Loss: 9356.195\n",
      "Iteration 040, Loss: 9698.303\n",
      "Iteration 050, Loss: 9894.069\n",
      "Iteration 060, Loss: 9554.902\n",
      "Iteration 070, Loss: 9041.620\n",
      "Iteration 080, Loss: 9132.491\n",
      "Iteration 090, Loss: 9251.062\n",
      "Iteration 100, Loss: 8662.121\n",
      "Iteration 110, Loss: 9767.469\n",
      "Iteration 120, Loss: 9502.228\n",
      "Iteration 130, Loss: 9282.034\n",
      "Iteration 140, Loss: 9528.351\n",
      "Iteration 150, Loss: 10572.517\n",
      "Iteration 160, Loss: 10126.062\n",
      "Iteration 170, Loss: 9538.972\n",
      "Iteration 180, Loss: 9504.943\n",
      "Iteration 190, Loss: 9665.629\n",
      "Iteration 199, Loss: 9673.873\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 8699.703\n",
      "Iteration 020, Loss: 8264.232\n",
      "Iteration 030, Loss: 8088.971\n",
      "Iteration 040, Loss: 8265.942\n",
      "Iteration 050, Loss: 8369.270\n",
      "Iteration 060, Loss: 8042.738\n",
      "Iteration 070, Loss: 7761.682\n",
      "Iteration 080, Loss: 7535.681\n",
      "Iteration 090, Loss: 7529.746\n",
      "Iteration 100, Loss: 7751.129\n",
      "Iteration 110, Loss: 8032.642\n",
      "Iteration 120, Loss: 8466.944\n",
      "Iteration 130, Loss: 8673.107\n",
      "Iteration 140, Loss: 8739.123\n",
      "Iteration 150, Loss: 8619.422\n",
      "Iteration 160, Loss: 8408.009\n",
      "Iteration 170, Loss: 8213.598\n",
      "Iteration 180, Loss: 8057.711\n",
      "Iteration 190, Loss: 7933.068\n",
      "Iteration 199, Loss: 7838.862\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 8656.213\n",
      "Iteration 020, Loss: 8167.912\n",
      "Iteration 030, Loss: 8154.083\n",
      "Iteration 040, Loss: 8351.452\n",
      "Iteration 050, Loss: 8260.312\n",
      "Iteration 060, Loss: 7851.802\n",
      "Iteration 070, Loss: 7596.557\n",
      "Iteration 080, Loss: 7511.811\n",
      "Iteration 090, Loss: 7746.575\n",
      "Iteration 100, Loss: 8076.378\n",
      "Iteration 110, Loss: 8545.180\n",
      "Iteration 120, Loss: 8740.241\n",
      "Iteration 130, Loss: 8754.515\n",
      "Iteration 140, Loss: 8514.149\n",
      "Iteration 150, Loss: 8281.199\n",
      "Iteration 160, Loss: 8115.358\n",
      "Iteration 170, Loss: 7946.392\n",
      "Iteration 180, Loss: 7825.272\n",
      "Iteration 190, Loss: 7687.084\n",
      "Iteration 199, Loss: 7643.126\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 8349.381\n",
      "Iteration 020, Loss: 8214.689\n",
      "Iteration 030, Loss: 8263.309\n",
      "Iteration 040, Loss: 7650.096\n",
      "Iteration 050, Loss: 7479.066\n",
      "Iteration 060, Loss: 8196.586\n",
      "Iteration 070, Loss: 8847.708\n",
      "Iteration 080, Loss: 8697.157\n",
      "Iteration 090, Loss: 8232.276\n",
      "Iteration 100, Loss: 8019.226\n",
      "Iteration 110, Loss: 7929.105\n",
      "Iteration 120, Loss: 7764.379\n",
      "Iteration 130, Loss: 7683.535\n",
      "Iteration 140, Loss: 7705.289\n",
      "Iteration 150, Loss: 7788.569\n",
      "Iteration 160, Loss: 7624.358\n",
      "Iteration 170, Loss: 7423.330\n",
      "Iteration 180, Loss: 7685.542\n",
      "Iteration 190, Loss: 7867.432\n",
      "Iteration 199, Loss: 8026.366\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 8183.906\n",
      "Iteration 020, Loss: 7989.558\n",
      "Iteration 030, Loss: 8187.779\n",
      "Iteration 040, Loss: 8948.013\n",
      "Iteration 050, Loss: 8286.845\n",
      "Iteration 060, Loss: 8295.776\n",
      "Iteration 070, Loss: 8884.785\n",
      "Iteration 080, Loss: 8352.305\n",
      "Iteration 090, Loss: 8366.425\n",
      "Iteration 100, Loss: 8873.838\n",
      "Iteration 110, Loss: 8626.664\n",
      "Iteration 120, Loss: 9508.558\n",
      "Iteration 130, Loss: 9226.134\n",
      "Iteration 140, Loss: 8845.272\n",
      "Iteration 150, Loss: 8312.533\n",
      "Iteration 160, Loss: 8845.830\n",
      "Iteration 170, Loss: 10244.757\n",
      "Iteration 180, Loss: 8904.596\n",
      "Iteration 190, Loss: 8618.000\n",
      "Iteration 199, Loss: 8311.693\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 8888.172\n",
      "Iteration 020, Loss: 8698.127\n",
      "Iteration 030, Loss: 8473.900\n",
      "Iteration 040, Loss: 8255.827\n",
      "Iteration 050, Loss: 8087.818\n",
      "Iteration 060, Loss: 8092.669\n",
      "Iteration 070, Loss: 8184.553\n",
      "Iteration 080, Loss: 8283.278\n",
      "Iteration 090, Loss: 8358.517\n",
      "Iteration 100, Loss: 8373.250\n",
      "Iteration 110, Loss: 8257.373\n",
      "Iteration 120, Loss: 8030.699\n",
      "Iteration 130, Loss: 7876.263\n",
      "Iteration 140, Loss: 7758.434\n",
      "Iteration 150, Loss: 7649.140\n",
      "Iteration 160, Loss: 7540.275\n",
      "Iteration 170, Loss: 7513.325\n",
      "Iteration 180, Loss: 7570.593\n",
      "Iteration 190, Loss: 7668.984\n",
      "Iteration 199, Loss: 7777.223\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 8870.585\n",
      "Iteration 020, Loss: 8657.146\n",
      "Iteration 030, Loss: 8399.598\n",
      "Iteration 040, Loss: 8153.271\n",
      "Iteration 050, Loss: 8062.690\n",
      "Iteration 060, Loss: 8150.277\n",
      "Iteration 070, Loss: 8257.965\n",
      "Iteration 080, Loss: 8351.764\n",
      "Iteration 090, Loss: 8373.250\n",
      "Iteration 100, Loss: 8238.807\n",
      "Iteration 110, Loss: 7993.156\n",
      "Iteration 120, Loss: 7842.243\n",
      "Iteration 130, Loss: 7710.672\n",
      "Iteration 140, Loss: 7584.108\n",
      "Iteration 150, Loss: 7513.386\n",
      "Iteration 160, Loss: 7543.351\n",
      "Iteration 170, Loss: 7645.029\n",
      "Iteration 180, Loss: 7781.418\n",
      "Iteration 190, Loss: 7907.024\n",
      "Iteration 199, Loss: 8114.414\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 8745.168\n",
      "Iteration 020, Loss: 8302.396\n",
      "Iteration 030, Loss: 8068.392\n",
      "Iteration 040, Loss: 8249.545\n",
      "Iteration 050, Loss: 8377.248\n",
      "Iteration 060, Loss: 8137.887\n",
      "Iteration 070, Loss: 7809.550\n",
      "Iteration 080, Loss: 7588.793\n",
      "Iteration 090, Loss: 7438.330\n",
      "Iteration 100, Loss: 7605.779\n",
      "Iteration 110, Loss: 7879.562\n",
      "Iteration 120, Loss: 8304.700\n",
      "Iteration 130, Loss: 8614.822\n",
      "Iteration 140, Loss: 8761.514\n",
      "Iteration 150, Loss: 8746.598\n",
      "Iteration 160, Loss: 8567.489\n",
      "Iteration 170, Loss: 8352.744\n",
      "Iteration 180, Loss: 8191.587\n",
      "Iteration 190, Loss: 8036.716\n",
      "Iteration 199, Loss: 7926.682\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 8278.974\n",
      "Iteration 020, Loss: 8643.263\n",
      "Iteration 030, Loss: 8102.306\n",
      "Iteration 040, Loss: 7471.586\n",
      "Iteration 050, Loss: 8291.119\n",
      "Iteration 060, Loss: 8337.888\n",
      "Iteration 070, Loss: 8526.675\n",
      "Iteration 080, Loss: 8081.128\n",
      "Iteration 090, Loss: 8457.100\n",
      "Iteration 100, Loss: 8818.149\n",
      "Iteration 110, Loss: 8667.742\n",
      "Iteration 120, Loss: 8603.257\n",
      "Iteration 130, Loss: 8412.500\n",
      "Iteration 140, Loss: 8361.035\n",
      "Iteration 150, Loss: 7609.949\n",
      "Iteration 160, Loss: 8382.749\n",
      "Iteration 170, Loss: 8133.802\n",
      "Iteration 180, Loss: 9209.968\n",
      "Iteration 190, Loss: 8584.735\n",
      "Iteration 199, Loss: 8492.564\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 9016.539\n",
      "Iteration 020, Loss: 8985.119\n",
      "Iteration 030, Loss: 8953.414\n",
      "Iteration 040, Loss: 8920.188\n",
      "Iteration 050, Loss: 8887.334\n",
      "Iteration 060, Loss: 8854.854\n",
      "Iteration 070, Loss: 8819.306\n",
      "Iteration 080, Loss: 8781.288\n",
      "Iteration 090, Loss: 8739.347\n",
      "Iteration 100, Loss: 8698.150\n",
      "Iteration 110, Loss: 8659.781\n",
      "Iteration 120, Loss: 8616.740\n",
      "Iteration 130, Loss: 8571.617\n",
      "Iteration 140, Loss: 8522.657\n",
      "Iteration 150, Loss: 8472.075\n",
      "Iteration 160, Loss: 8425.562\n",
      "Iteration 170, Loss: 8379.944\n",
      "Iteration 180, Loss: 8337.843\n",
      "Iteration 190, Loss: 8295.678\n",
      "Iteration 199, Loss: 8254.154\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 9013.508\n",
      "Iteration 020, Loss: 8978.611\n",
      "Iteration 030, Loss: 8942.947\n",
      "Iteration 040, Loss: 8905.952\n",
      "Iteration 050, Loss: 8869.795\n",
      "Iteration 060, Loss: 8831.984\n",
      "Iteration 070, Loss: 8790.368\n",
      "Iteration 080, Loss: 8744.247\n",
      "Iteration 090, Loss: 8698.200\n",
      "Iteration 100, Loss: 8655.434\n",
      "Iteration 110, Loss: 8606.599\n",
      "Iteration 120, Loss: 8556.817\n",
      "Iteration 130, Loss: 8499.270\n",
      "Iteration 140, Loss: 8445.547\n",
      "Iteration 150, Loss: 8394.685\n",
      "Iteration 160, Loss: 8347.521\n",
      "Iteration 170, Loss: 8300.935\n",
      "Iteration 180, Loss: 8249.798\n",
      "Iteration 190, Loss: 8198.300\n",
      "Iteration 199, Loss: 8149.933\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 8990.475\n",
      "Iteration 020, Loss: 8926.708\n",
      "Iteration 030, Loss: 8861.172\n",
      "Iteration 040, Loss: 8789.815\n",
      "Iteration 050, Loss: 8706.524\n",
      "Iteration 060, Loss: 8627.383\n",
      "Iteration 070, Loss: 8536.262\n",
      "Iteration 080, Loss: 8435.943\n",
      "Iteration 090, Loss: 8348.787\n",
      "Iteration 100, Loss: 8265.624\n",
      "Iteration 110, Loss: 8171.652\n",
      "Iteration 120, Loss: 8099.543\n",
      "Iteration 130, Loss: 8083.766\n",
      "Iteration 140, Loss: 8065.800\n",
      "Iteration 150, Loss: 8091.256\n",
      "Iteration 160, Loss: 8126.370\n",
      "Iteration 170, Loss: 8159.891\n",
      "Iteration 180, Loss: 8198.716\n",
      "Iteration 190, Loss: 8241.267\n",
      "Iteration 199, Loss: 8276.195\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 8911.401\n",
      "Iteration 020, Loss: 8622.917\n",
      "Iteration 030, Loss: 8230.896\n",
      "Iteration 040, Loss: 8057.067\n",
      "Iteration 050, Loss: 8218.953\n",
      "Iteration 060, Loss: 8367.711\n",
      "Iteration 070, Loss: 8353.645\n",
      "Iteration 080, Loss: 8006.790\n",
      "Iteration 090, Loss: 7768.946\n",
      "Iteration 100, Loss: 7588.278\n",
      "Iteration 110, Loss: 7589.667\n",
      "Iteration 120, Loss: 7560.063\n",
      "Iteration 130, Loss: 7632.112\n",
      "Iteration 140, Loss: 7964.875\n",
      "Iteration 150, Loss: 8623.316\n",
      "Iteration 160, Loss: 9095.762\n",
      "Iteration 170, Loss: 9373.265\n",
      "Iteration 180, Loss: 9512.308\n",
      "Iteration 190, Loss: 9498.348\n",
      "Iteration 199, Loss: 9102.640\n"
     ]
    }
   ],
   "source": [
    "optimized23 = {}\n",
    "for p in params:\n",
    "    s = optimize('1z6mA02', random_state=3, iterations=200, lr=p[0], \n",
    "                 momentum=p[1], normal=True, angle_potential=False, normalize_gradients=False,\n",
    "                scale_gradients=True)\n",
    "    optimized23[f'{p}'] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('1z6mA02_trials/1z6m_smooth_scaled_23.pkl', 'wb') as f:\n",
    "    pickle.dump(optimized23, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
