{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "from optimize import optimize\n",
    "from structure import Structure\n",
    "\n",
    "params = []\n",
    "for l in [0.1, 0.05, 0.01, 0.005, 0.001]:\n",
    "    for m in [0, 0.1, 0.5, 0.9]:\n",
    "        params.append((l, m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 8554.959\n",
      "Iteration 020, Loss: 8689.837\n",
      "Iteration 030, Loss: 8066.767\n",
      "Iteration 040, Loss: 8149.347\n",
      "Iteration 050, Loss: 7696.985\n",
      "Iteration 060, Loss: 8736.516\n",
      "Iteration 070, Loss: 8991.312\n",
      "Iteration 080, Loss: 9287.438\n",
      "Iteration 090, Loss: 8402.218\n",
      "Iteration 100, Loss: 8521.301\n",
      "Iteration 110, Loss: 8699.190\n",
      "Iteration 120, Loss: 7899.305\n",
      "Iteration 130, Loss: 9130.498\n",
      "Iteration 140, Loss: 9200.560\n",
      "Iteration 150, Loss: 7571.846\n",
      "Iteration 160, Loss: 9292.901\n",
      "Iteration 170, Loss: 7530.532\n",
      "Iteration 180, Loss: 9487.013\n",
      "Iteration 190, Loss: 8388.004\n",
      "Iteration 199, Loss: 8599.938\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 10247.925\n",
      "Iteration 020, Loss: 7817.922\n",
      "Iteration 030, Loss: 8429.660\n",
      "Iteration 040, Loss: 8073.582\n",
      "Iteration 050, Loss: 7961.270\n",
      "Iteration 060, Loss: 8359.047\n",
      "Iteration 070, Loss: 8275.264\n",
      "Iteration 080, Loss: 7927.980\n",
      "Iteration 090, Loss: 8968.023\n",
      "Iteration 100, Loss: 9172.770\n",
      "Iteration 110, Loss: 8258.583\n",
      "Iteration 120, Loss: 8020.323\n",
      "Iteration 130, Loss: 8233.458\n",
      "Iteration 140, Loss: 8997.592\n",
      "Iteration 150, Loss: 8840.219\n",
      "Iteration 160, Loss: 8250.711\n",
      "Iteration 170, Loss: 8326.906\n",
      "Iteration 180, Loss: 8732.274\n",
      "Iteration 190, Loss: 8383.896\n",
      "Iteration 199, Loss: 9337.639\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 9670.915\n",
      "Iteration 020, Loss: 8584.869\n",
      "Iteration 030, Loss: 8132.370\n",
      "Iteration 040, Loss: 8958.209\n",
      "Iteration 050, Loss: 9263.198\n",
      "Iteration 060, Loss: 10128.654\n",
      "Iteration 070, Loss: 8814.566\n",
      "Iteration 080, Loss: 10020.799\n",
      "Iteration 090, Loss: 9243.729\n",
      "Iteration 100, Loss: 9435.409\n",
      "Iteration 110, Loss: 8416.487\n",
      "Iteration 120, Loss: 9201.704\n",
      "Iteration 130, Loss: 8511.895\n",
      "Iteration 140, Loss: 8902.431\n",
      "Iteration 150, Loss: 8610.344\n",
      "Iteration 160, Loss: 9094.773\n",
      "Iteration 170, Loss: 9204.224\n",
      "Iteration 180, Loss: 8310.792\n",
      "Iteration 190, Loss: 9896.007\n",
      "Iteration 199, Loss: 9162.300\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 10560.928\n",
      "Iteration 020, Loss: 10967.962\n",
      "Iteration 030, Loss: 10008.414\n",
      "Iteration 040, Loss: 10414.104\n",
      "Iteration 050, Loss: 10113.834\n",
      "Iteration 060, Loss: 9629.118\n",
      "Iteration 070, Loss: 10334.317\n",
      "Iteration 080, Loss: 11966.512\n",
      "Iteration 090, Loss: 11222.793\n",
      "Iteration 100, Loss: 10040.062\n",
      "Iteration 110, Loss: 12641.582\n",
      "Iteration 120, Loss: 11600.470\n",
      "Iteration 130, Loss: 11763.962\n",
      "Iteration 140, Loss: 12466.105\n",
      "Iteration 150, Loss: 10407.406\n",
      "Iteration 160, Loss: 10177.202\n",
      "Iteration 170, Loss: 12694.024\n",
      "Iteration 180, Loss: 9601.202\n",
      "Iteration 190, Loss: 10916.315\n",
      "Iteration 199, Loss: 9356.758\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 9471.154\n",
      "Iteration 020, Loss: 8524.992\n",
      "Iteration 030, Loss: 8027.055\n",
      "Iteration 040, Loss: 8190.821\n",
      "Iteration 050, Loss: 8539.887\n",
      "Iteration 060, Loss: 8385.618\n",
      "Iteration 070, Loss: 8144.164\n",
      "Iteration 080, Loss: 7922.819\n",
      "Iteration 090, Loss: 7526.934\n",
      "Iteration 100, Loss: 8387.491\n",
      "Iteration 110, Loss: 8839.434\n",
      "Iteration 120, Loss: 8976.353\n",
      "Iteration 130, Loss: 8548.984\n",
      "Iteration 140, Loss: 8808.359\n",
      "Iteration 150, Loss: 8640.017\n",
      "Iteration 160, Loss: 8269.354\n",
      "Iteration 170, Loss: 8021.125\n",
      "Iteration 180, Loss: 8446.405\n",
      "Iteration 190, Loss: 7758.495\n",
      "Iteration 199, Loss: 7341.702\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 9160.285\n",
      "Iteration 020, Loss: 8551.269\n",
      "Iteration 030, Loss: 8777.281\n",
      "Iteration 040, Loss: 8942.120\n",
      "Iteration 050, Loss: 7996.311\n",
      "Iteration 060, Loss: 9059.000\n",
      "Iteration 070, Loss: 8216.269\n",
      "Iteration 080, Loss: 7653.122\n",
      "Iteration 090, Loss: 9025.086\n",
      "Iteration 100, Loss: 9506.524\n",
      "Iteration 110, Loss: 8562.478\n",
      "Iteration 120, Loss: 8560.640\n",
      "Iteration 130, Loss: 8307.847\n",
      "Iteration 140, Loss: 8779.991\n",
      "Iteration 150, Loss: 8471.819\n",
      "Iteration 160, Loss: 7725.651\n",
      "Iteration 170, Loss: 7884.663\n",
      "Iteration 180, Loss: 8132.050\n",
      "Iteration 190, Loss: 8824.221\n",
      "Iteration 199, Loss: 7875.712\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 9845.983\n",
      "Iteration 020, Loss: 7873.881\n",
      "Iteration 030, Loss: 7879.623\n",
      "Iteration 040, Loss: 7830.171\n",
      "Iteration 050, Loss: 9172.424\n",
      "Iteration 060, Loss: 8350.464\n",
      "Iteration 070, Loss: 8451.139\n",
      "Iteration 080, Loss: 8736.544\n",
      "Iteration 090, Loss: 8249.465\n",
      "Iteration 100, Loss: 8570.668\n",
      "Iteration 110, Loss: 7932.620\n",
      "Iteration 120, Loss: 8695.254\n",
      "Iteration 130, Loss: 8112.569\n",
      "Iteration 140, Loss: 8560.213\n",
      "Iteration 150, Loss: 9827.923\n",
      "Iteration 160, Loss: 8531.919\n",
      "Iteration 170, Loss: 7906.579\n",
      "Iteration 180, Loss: 8415.123\n",
      "Iteration 190, Loss: 8945.154\n",
      "Iteration 199, Loss: 9343.923\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 9726.607\n",
      "Iteration 020, Loss: 9306.932\n",
      "Iteration 030, Loss: 10922.376\n",
      "Iteration 040, Loss: 12187.635\n",
      "Iteration 050, Loss: 9769.412\n",
      "Iteration 060, Loss: 9619.696\n",
      "Iteration 070, Loss: 9387.644\n",
      "Iteration 080, Loss: 10226.562\n",
      "Iteration 090, Loss: 10418.682\n",
      "Iteration 100, Loss: 9482.883\n",
      "Iteration 110, Loss: 10101.094\n",
      "Iteration 120, Loss: 9431.896\n",
      "Iteration 130, Loss: 9453.572\n",
      "Iteration 140, Loss: 10780.257\n",
      "Iteration 150, Loss: 9558.980\n",
      "Iteration 160, Loss: 9626.691\n",
      "Iteration 170, Loss: 10290.347\n",
      "Iteration 180, Loss: 9361.234\n",
      "Iteration 190, Loss: 10606.533\n",
      "Iteration 199, Loss: 9638.243\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 9604.864\n",
      "Iteration 020, Loss: 8448.586\n",
      "Iteration 030, Loss: 8458.312\n",
      "Iteration 040, Loss: 8494.271\n",
      "Iteration 050, Loss: 8329.908\n",
      "Iteration 060, Loss: 8425.660\n",
      "Iteration 070, Loss: 8223.691\n",
      "Iteration 080, Loss: 8088.189\n",
      "Iteration 090, Loss: 7784.146\n",
      "Iteration 100, Loss: 7735.445\n",
      "Iteration 110, Loss: 7330.397\n",
      "Iteration 120, Loss: 7285.930\n",
      "Iteration 130, Loss: 7436.474\n",
      "Iteration 140, Loss: 7419.996\n",
      "Iteration 150, Loss: 8112.182\n",
      "Iteration 160, Loss: 9368.037\n",
      "Iteration 170, Loss: 8685.422\n",
      "Iteration 180, Loss: 8385.628\n",
      "Iteration 190, Loss: 8029.715\n",
      "Iteration 199, Loss: 7777.071\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 9553.774\n",
      "Iteration 020, Loss: 8247.358\n",
      "Iteration 030, Loss: 8515.600\n",
      "Iteration 040, Loss: 8380.678\n",
      "Iteration 050, Loss: 8463.309\n",
      "Iteration 060, Loss: 8400.685\n",
      "Iteration 070, Loss: 8156.492\n",
      "Iteration 080, Loss: 7815.376\n",
      "Iteration 090, Loss: 7640.667\n",
      "Iteration 100, Loss: 7527.964\n",
      "Iteration 110, Loss: 7943.304\n",
      "Iteration 120, Loss: 8573.313\n",
      "Iteration 130, Loss: 8699.884\n",
      "Iteration 140, Loss: 7994.085\n",
      "Iteration 150, Loss: 7654.007\n",
      "Iteration 160, Loss: 8045.906\n",
      "Iteration 170, Loss: 7703.360\n",
      "Iteration 180, Loss: 7086.004\n",
      "Iteration 190, Loss: 6908.971\n",
      "Iteration 199, Loss: 7062.894\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 9030.976\n",
      "Iteration 020, Loss: 8667.634\n",
      "Iteration 030, Loss: 8533.255\n",
      "Iteration 040, Loss: 8748.907\n",
      "Iteration 050, Loss: 8794.240\n",
      "Iteration 060, Loss: 7744.096\n",
      "Iteration 070, Loss: 7383.205\n",
      "Iteration 080, Loss: 7199.429\n",
      "Iteration 090, Loss: 7221.028\n",
      "Iteration 100, Loss: 7562.923\n",
      "Iteration 110, Loss: 7825.372\n",
      "Iteration 120, Loss: 8266.223\n",
      "Iteration 130, Loss: 8139.817\n",
      "Iteration 140, Loss: 8003.729\n",
      "Iteration 150, Loss: 7847.180\n",
      "Iteration 160, Loss: 8011.712\n",
      "Iteration 170, Loss: 8059.395\n",
      "Iteration 180, Loss: 7872.942\n",
      "Iteration 190, Loss: 7854.343\n",
      "Iteration 199, Loss: 8300.213\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 8940.850\n",
      "Iteration 020, Loss: 8845.656\n",
      "Iteration 030, Loss: 8880.190\n",
      "Iteration 040, Loss: 8128.927\n",
      "Iteration 050, Loss: 9049.151\n",
      "Iteration 060, Loss: 9288.253\n",
      "Iteration 070, Loss: 8749.223\n",
      "Iteration 080, Loss: 8696.470\n",
      "Iteration 090, Loss: 10645.678\n",
      "Iteration 100, Loss: 10552.790\n",
      "Iteration 110, Loss: 9565.827\n",
      "Iteration 120, Loss: 8804.375\n",
      "Iteration 130, Loss: 9041.864\n",
      "Iteration 140, Loss: 9648.883\n",
      "Iteration 150, Loss: 9775.530\n",
      "Iteration 160, Loss: 8768.062\n",
      "Iteration 170, Loss: 10549.718\n",
      "Iteration 180, Loss: 9176.642\n",
      "Iteration 190, Loss: 9565.577\n",
      "Iteration 199, Loss: 10185.188\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 10633.357\n",
      "Iteration 020, Loss: 9613.301\n",
      "Iteration 030, Loss: 9109.145\n",
      "Iteration 040, Loss: 8466.809\n",
      "Iteration 050, Loss: 8268.871\n",
      "Iteration 060, Loss: 8438.732\n",
      "Iteration 070, Loss: 8490.986\n",
      "Iteration 080, Loss: 8432.786\n",
      "Iteration 090, Loss: 8339.653\n",
      "Iteration 100, Loss: 8372.916\n",
      "Iteration 110, Loss: 8437.486\n",
      "Iteration 120, Loss: 8456.418\n",
      "Iteration 130, Loss: 8360.718\n",
      "Iteration 140, Loss: 8222.804\n",
      "Iteration 150, Loss: 8119.258\n",
      "Iteration 160, Loss: 8078.142\n",
      "Iteration 170, Loss: 7927.552\n",
      "Iteration 180, Loss: 7649.151\n",
      "Iteration 190, Loss: 7535.077\n",
      "Iteration 199, Loss: 7388.815\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 10523.479\n",
      "Iteration 020, Loss: 9541.758\n",
      "Iteration 030, Loss: 8958.956\n",
      "Iteration 040, Loss: 8271.062\n",
      "Iteration 050, Loss: 8373.043\n",
      "Iteration 060, Loss: 8502.848\n",
      "Iteration 070, Loss: 8453.850\n",
      "Iteration 080, Loss: 8348.202\n",
      "Iteration 090, Loss: 8384.310\n",
      "Iteration 100, Loss: 8463.869\n",
      "Iteration 110, Loss: 8487.322\n",
      "Iteration 120, Loss: 8346.750\n",
      "Iteration 130, Loss: 8185.101\n",
      "Iteration 140, Loss: 8128.954\n",
      "Iteration 150, Loss: 8076.043\n",
      "Iteration 160, Loss: 7751.960\n",
      "Iteration 170, Loss: 7501.662\n",
      "Iteration 180, Loss: 7503.515\n",
      "Iteration 190, Loss: 7598.300\n",
      "Iteration 199, Loss: 7964.880\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 9682.136\n",
      "Iteration 020, Loss: 8724.194\n",
      "Iteration 030, Loss: 8376.193\n",
      "Iteration 040, Loss: 8578.663\n",
      "Iteration 050, Loss: 8424.800\n",
      "Iteration 060, Loss: 8495.127\n",
      "Iteration 070, Loss: 8298.655\n",
      "Iteration 080, Loss: 8201.008\n",
      "Iteration 090, Loss: 8444.465\n",
      "Iteration 100, Loss: 8434.628\n",
      "Iteration 110, Loss: 8320.323\n",
      "Iteration 120, Loss: 8471.510\n",
      "Iteration 130, Loss: 8065.302\n",
      "Iteration 140, Loss: 8317.843\n",
      "Iteration 150, Loss: 8295.849\n",
      "Iteration 160, Loss: 7567.344\n",
      "Iteration 170, Loss: 7921.598\n",
      "Iteration 180, Loss: 8201.301\n",
      "Iteration 190, Loss: 8410.717\n",
      "Iteration 199, Loss: 8448.375\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 8635.805\n",
      "Iteration 020, Loss: 8987.469\n",
      "Iteration 030, Loss: 8736.646\n",
      "Iteration 040, Loss: 8342.738\n",
      "Iteration 050, Loss: 7243.334\n",
      "Iteration 060, Loss: 7526.808\n",
      "Iteration 070, Loss: 9059.429\n",
      "Iteration 080, Loss: 7978.833\n",
      "Iteration 090, Loss: 8730.928\n",
      "Iteration 100, Loss: 8726.093\n",
      "Iteration 110, Loss: 8290.275\n",
      "Iteration 120, Loss: 9098.721\n",
      "Iteration 130, Loss: 8755.862\n",
      "Iteration 140, Loss: 9213.098\n",
      "Iteration 150, Loss: 9809.994\n",
      "Iteration 160, Loss: 9840.751\n",
      "Iteration 170, Loss: 9142.148\n",
      "Iteration 180, Loss: 9269.931\n",
      "Iteration 190, Loss: 8680.950\n",
      "Iteration 199, Loss: 8832.311\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 11266.442\n",
      "Iteration 020, Loss: 11156.469\n",
      "Iteration 030, Loss: 11002.471\n",
      "Iteration 040, Loss: 10814.319\n",
      "Iteration 050, Loss: 10624.405\n",
      "Iteration 060, Loss: 10358.598\n",
      "Iteration 070, Loss: 10081.701\n",
      "Iteration 080, Loss: 9868.111\n",
      "Iteration 090, Loss: 9698.240\n",
      "Iteration 100, Loss: 9615.566\n",
      "Iteration 110, Loss: 9547.671\n",
      "Iteration 120, Loss: 9469.935\n",
      "Iteration 130, Loss: 9404.914\n",
      "Iteration 140, Loss: 9303.231\n",
      "Iteration 150, Loss: 9122.228\n",
      "Iteration 160, Loss: 9002.179\n",
      "Iteration 170, Loss: 8921.053\n",
      "Iteration 180, Loss: 8788.841\n",
      "Iteration 190, Loss: 8622.725\n",
      "Iteration 199, Loss: 8502.283\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 11256.247\n",
      "Iteration 020, Loss: 11128.411\n",
      "Iteration 030, Loss: 10949.930\n",
      "Iteration 040, Loss: 10733.425\n",
      "Iteration 050, Loss: 10497.194\n",
      "Iteration 060, Loss: 10181.298\n",
      "Iteration 070, Loss: 9912.285\n",
      "Iteration 080, Loss: 9713.869\n",
      "Iteration 090, Loss: 9615.481\n",
      "Iteration 100, Loss: 9540.755\n",
      "Iteration 110, Loss: 9455.517\n",
      "Iteration 120, Loss: 9377.769\n",
      "Iteration 130, Loss: 9232.778\n",
      "Iteration 140, Loss: 9044.090\n",
      "Iteration 150, Loss: 8957.976\n",
      "Iteration 160, Loss: 8826.708\n",
      "Iteration 170, Loss: 8640.828\n",
      "Iteration 180, Loss: 8492.848\n",
      "Iteration 190, Loss: 8374.744\n",
      "Iteration 199, Loss: 8301.726\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 11184.938\n",
      "Iteration 020, Loss: 10867.722\n",
      "Iteration 030, Loss: 10439.688\n",
      "Iteration 040, Loss: 9902.228\n",
      "Iteration 050, Loss: 9622.305\n",
      "Iteration 060, Loss: 9484.307\n",
      "Iteration 070, Loss: 9330.033\n",
      "Iteration 080, Loss: 9017.788\n",
      "Iteration 090, Loss: 8808.196\n",
      "Iteration 100, Loss: 8494.571\n",
      "Iteration 110, Loss: 8300.346\n",
      "Iteration 120, Loss: 8254.404\n",
      "Iteration 130, Loss: 8296.708\n",
      "Iteration 140, Loss: 8382.115\n",
      "Iteration 150, Loss: 8437.176\n",
      "Iteration 160, Loss: 8479.678\n",
      "Iteration 170, Loss: 8498.042\n",
      "Iteration 180, Loss: 8488.490\n",
      "Iteration 190, Loss: 8462.040\n",
      "Iteration 199, Loss: 8439.282\n",
      "Iteration 000, Loss: 11356.329\n",
      "Iteration 010, Loss: 10886.263\n",
      "Iteration 020, Loss: 9575.411\n",
      "Iteration 030, Loss: 8351.854\n",
      "Iteration 040, Loss: 8011.018\n",
      "Iteration 050, Loss: 8252.988\n",
      "Iteration 060, Loss: 8678.977\n",
      "Iteration 070, Loss: 9486.051\n",
      "Iteration 080, Loss: 9393.806\n",
      "Iteration 090, Loss: 8997.046\n",
      "Iteration 100, Loss: 8808.965\n",
      "Iteration 110, Loss: 8640.036\n",
      "Iteration 120, Loss: 8425.582\n",
      "Iteration 130, Loss: 8224.213\n",
      "Iteration 140, Loss: 8088.950\n",
      "Iteration 150, Loss: 7969.246\n",
      "Iteration 160, Loss: 8081.155\n",
      "Iteration 170, Loss: 8167.149\n",
      "Iteration 180, Loss: 8430.671\n",
      "Iteration 190, Loss: 8513.429\n",
      "Iteration 199, Loss: 8606.591\n"
     ]
    }
   ],
   "source": [
    "optimized3 = {}\n",
    "for p in params:\n",
    "    s = optimize('1z6mA02', random_state=1, iterations=200, lr=p[0], \n",
    "                 momentum=p[1], normal=True, angle_potential=False, normalize_gradients=False,\n",
    "                scale_gradients=False, normalize_gradients_2=True)\n",
    "    optimized3[f'{p}'] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('1z6mA02_trials/1z6m_smooth_standdev_31.pkl', 'wb') as f:\n",
    "    pickle.dump(optimized3, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 8436.333\n",
      "Iteration 020, Loss: 8307.449\n",
      "Iteration 030, Loss: 8700.860\n",
      "Iteration 040, Loss: 8218.646\n",
      "Iteration 050, Loss: 8712.076\n",
      "Iteration 060, Loss: 8951.251\n",
      "Iteration 070, Loss: 7711.120\n",
      "Iteration 080, Loss: 7296.747\n",
      "Iteration 090, Loss: 7937.988\n",
      "Iteration 100, Loss: 9206.598\n",
      "Iteration 110, Loss: 9489.811\n",
      "Iteration 120, Loss: 7843.976\n",
      "Iteration 130, Loss: 8786.446\n",
      "Iteration 140, Loss: 8997.082\n",
      "Iteration 150, Loss: 8390.149\n",
      "Iteration 160, Loss: 8070.898\n",
      "Iteration 170, Loss: 9187.489\n",
      "Iteration 180, Loss: 8999.991\n",
      "Iteration 190, Loss: 8708.066\n",
      "Iteration 199, Loss: 8290.765\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 7784.548\n",
      "Iteration 020, Loss: 7845.567\n",
      "Iteration 030, Loss: 7317.232\n",
      "Iteration 040, Loss: 9042.020\n",
      "Iteration 050, Loss: 8559.156\n",
      "Iteration 060, Loss: 8492.158\n",
      "Iteration 070, Loss: 8148.736\n",
      "Iteration 080, Loss: 7388.104\n",
      "Iteration 090, Loss: 8497.069\n",
      "Iteration 100, Loss: 8390.678\n",
      "Iteration 110, Loss: 8255.423\n",
      "Iteration 120, Loss: 7884.683\n",
      "Iteration 130, Loss: 8061.742\n",
      "Iteration 140, Loss: 7633.600\n",
      "Iteration 150, Loss: 8071.436\n",
      "Iteration 160, Loss: 7688.748\n",
      "Iteration 170, Loss: 8335.264\n",
      "Iteration 180, Loss: 7893.246\n",
      "Iteration 190, Loss: 7356.221\n",
      "Iteration 199, Loss: 8071.748\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 9322.899\n",
      "Iteration 020, Loss: 8553.514\n",
      "Iteration 030, Loss: 8485.929\n",
      "Iteration 040, Loss: 8290.586\n",
      "Iteration 050, Loss: 7646.759\n",
      "Iteration 060, Loss: 9248.156\n",
      "Iteration 070, Loss: 7744.751\n",
      "Iteration 080, Loss: 8162.569\n",
      "Iteration 090, Loss: 8192.237\n",
      "Iteration 100, Loss: 7620.519\n",
      "Iteration 110, Loss: 7877.280\n",
      "Iteration 120, Loss: 8527.292\n",
      "Iteration 130, Loss: 8157.746\n",
      "Iteration 140, Loss: 8967.209\n",
      "Iteration 150, Loss: 9139.185\n",
      "Iteration 160, Loss: 8092.098\n",
      "Iteration 170, Loss: 9427.536\n",
      "Iteration 180, Loss: 9025.322\n",
      "Iteration 190, Loss: 9084.027\n",
      "Iteration 199, Loss: 8843.244\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 13601.699\n",
      "Iteration 020, Loss: 10243.423\n",
      "Iteration 030, Loss: 10607.405\n",
      "Iteration 040, Loss: 9975.559\n",
      "Iteration 050, Loss: 10389.700\n",
      "Iteration 060, Loss: 12919.779\n",
      "Iteration 070, Loss: 9309.445\n",
      "Iteration 080, Loss: 10538.033\n",
      "Iteration 090, Loss: 11772.692\n",
      "Iteration 100, Loss: 9935.570\n",
      "Iteration 110, Loss: 9989.721\n",
      "Iteration 120, Loss: 10293.201\n",
      "Iteration 130, Loss: 9976.004\n",
      "Iteration 140, Loss: 11408.136\n",
      "Iteration 150, Loss: 10705.888\n",
      "Iteration 160, Loss: 10089.234\n",
      "Iteration 170, Loss: 9418.467\n",
      "Iteration 180, Loss: 12156.937\n",
      "Iteration 190, Loss: 9330.129\n",
      "Iteration 199, Loss: 11684.539\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 11403.476\n",
      "Iteration 020, Loss: 7669.742\n",
      "Iteration 030, Loss: 7345.093\n",
      "Iteration 040, Loss: 7459.143\n",
      "Iteration 050, Loss: 7526.634\n",
      "Iteration 060, Loss: 7563.369\n",
      "Iteration 070, Loss: 7759.758\n",
      "Iteration 080, Loss: 8326.472\n",
      "Iteration 090, Loss: 8455.304\n",
      "Iteration 100, Loss: 8765.623\n",
      "Iteration 110, Loss: 8078.676\n",
      "Iteration 120, Loss: 8023.316\n",
      "Iteration 130, Loss: 7278.306\n",
      "Iteration 140, Loss: 7366.915\n",
      "Iteration 150, Loss: 7582.135\n",
      "Iteration 160, Loss: 8346.062\n",
      "Iteration 170, Loss: 8371.712\n",
      "Iteration 180, Loss: 8612.998\n",
      "Iteration 190, Loss: 8655.626\n",
      "Iteration 199, Loss: 8618.923\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 10560.982\n",
      "Iteration 020, Loss: 7665.460\n",
      "Iteration 030, Loss: 7080.104\n",
      "Iteration 040, Loss: 7632.889\n",
      "Iteration 050, Loss: 7907.821\n",
      "Iteration 060, Loss: 7766.257\n",
      "Iteration 070, Loss: 7770.150\n",
      "Iteration 080, Loss: 8145.694\n",
      "Iteration 090, Loss: 8733.509\n",
      "Iteration 100, Loss: 8239.607\n",
      "Iteration 110, Loss: 8454.663\n",
      "Iteration 120, Loss: 9168.536\n",
      "Iteration 130, Loss: 8485.098\n",
      "Iteration 140, Loss: 8398.688\n",
      "Iteration 150, Loss: 8792.620\n",
      "Iteration 160, Loss: 8546.414\n",
      "Iteration 170, Loss: 8823.097\n",
      "Iteration 180, Loss: 8322.973\n",
      "Iteration 190, Loss: 7973.149\n",
      "Iteration 199, Loss: 7816.756\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 8658.196\n",
      "Iteration 020, Loss: 8201.165\n",
      "Iteration 030, Loss: 9274.301\n",
      "Iteration 040, Loss: 8715.033\n",
      "Iteration 050, Loss: 7797.188\n",
      "Iteration 060, Loss: 8514.056\n",
      "Iteration 070, Loss: 8647.199\n",
      "Iteration 080, Loss: 8400.683\n",
      "Iteration 090, Loss: 9731.525\n",
      "Iteration 100, Loss: 8539.180\n",
      "Iteration 110, Loss: 8294.466\n",
      "Iteration 120, Loss: 9112.817\n",
      "Iteration 130, Loss: 8398.979\n",
      "Iteration 140, Loss: 8691.692\n",
      "Iteration 150, Loss: 7814.508\n",
      "Iteration 160, Loss: 8049.230\n",
      "Iteration 170, Loss: 9279.191\n",
      "Iteration 180, Loss: 7861.902\n",
      "Iteration 190, Loss: 8774.302\n",
      "Iteration 199, Loss: 9363.602\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 10287.468\n",
      "Iteration 020, Loss: 10262.708\n",
      "Iteration 030, Loss: 9067.001\n",
      "Iteration 040, Loss: 9281.391\n",
      "Iteration 050, Loss: 9902.914\n",
      "Iteration 060, Loss: 9987.729\n",
      "Iteration 070, Loss: 11630.714\n",
      "Iteration 080, Loss: 10181.749\n",
      "Iteration 090, Loss: 9397.895\n",
      "Iteration 100, Loss: 11406.517\n",
      "Iteration 110, Loss: 10208.762\n",
      "Iteration 120, Loss: 10281.726\n",
      "Iteration 130, Loss: 11595.683\n",
      "Iteration 140, Loss: 12072.194\n",
      "Iteration 150, Loss: 10195.006\n",
      "Iteration 160, Loss: 9651.692\n",
      "Iteration 170, Loss: 9352.361\n",
      "Iteration 180, Loss: 11752.626\n",
      "Iteration 190, Loss: 10003.119\n",
      "Iteration 199, Loss: 9014.836\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 15338.022\n",
      "Iteration 020, Loss: 15011.071\n",
      "Iteration 030, Loss: 14275.188\n",
      "Iteration 040, Loss: 12852.961\n",
      "Iteration 050, Loss: 11379.311\n",
      "Iteration 060, Loss: 10385.645\n",
      "Iteration 070, Loss: 8444.252\n",
      "Iteration 080, Loss: 7427.096\n",
      "Iteration 090, Loss: 7186.732\n",
      "Iteration 100, Loss: 6997.083\n",
      "Iteration 110, Loss: 6909.356\n",
      "Iteration 120, Loss: 7140.474\n",
      "Iteration 130, Loss: 7344.957\n",
      "Iteration 140, Loss: 7664.766\n",
      "Iteration 150, Loss: 7614.292\n",
      "Iteration 160, Loss: 7592.042\n",
      "Iteration 170, Loss: 7485.323\n",
      "Iteration 180, Loss: 7339.343\n",
      "Iteration 190, Loss: 7402.559\n",
      "Iteration 199, Loss: 7380.420\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 15313.932\n",
      "Iteration 020, Loss: 14897.125\n",
      "Iteration 030, Loss: 13923.030\n",
      "Iteration 040, Loss: 12289.969\n",
      "Iteration 050, Loss: 10547.217\n",
      "Iteration 060, Loss: 9592.403\n",
      "Iteration 070, Loss: 7358.219\n",
      "Iteration 080, Loss: 7298.848\n",
      "Iteration 090, Loss: 7075.372\n",
      "Iteration 100, Loss: 7240.508\n",
      "Iteration 110, Loss: 7179.264\n",
      "Iteration 120, Loss: 7192.371\n",
      "Iteration 130, Loss: 7146.642\n",
      "Iteration 140, Loss: 7171.925\n",
      "Iteration 150, Loss: 7335.733\n",
      "Iteration 160, Loss: 7495.301\n",
      "Iteration 170, Loss: 7294.176\n",
      "Iteration 180, Loss: 7567.421\n",
      "Iteration 190, Loss: 8831.531\n",
      "Iteration 199, Loss: 8292.160\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 15105.206\n",
      "Iteration 020, Loss: 13459.548\n",
      "Iteration 030, Loss: 10222.363\n",
      "Iteration 040, Loss: 7293.378\n",
      "Iteration 050, Loss: 7587.875\n",
      "Iteration 060, Loss: 7302.336\n",
      "Iteration 070, Loss: 7060.950\n",
      "Iteration 080, Loss: 7276.829\n",
      "Iteration 090, Loss: 7318.838\n",
      "Iteration 100, Loss: 6940.876\n",
      "Iteration 110, Loss: 6859.626\n",
      "Iteration 120, Loss: 7313.900\n",
      "Iteration 130, Loss: 7629.395\n",
      "Iteration 140, Loss: 8160.682\n",
      "Iteration 150, Loss: 9844.415\n",
      "Iteration 160, Loss: 9587.968\n",
      "Iteration 170, Loss: 8444.356\n",
      "Iteration 180, Loss: 8158.013\n",
      "Iteration 190, Loss: 7804.321\n",
      "Iteration 199, Loss: 7520.310\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 13402.315\n",
      "Iteration 020, Loss: 9388.304\n",
      "Iteration 030, Loss: 9060.325\n",
      "Iteration 040, Loss: 8993.033\n",
      "Iteration 050, Loss: 8441.094\n",
      "Iteration 060, Loss: 8463.839\n",
      "Iteration 070, Loss: 8278.260\n",
      "Iteration 080, Loss: 7939.521\n",
      "Iteration 090, Loss: 8851.377\n",
      "Iteration 100, Loss: 9134.324\n",
      "Iteration 110, Loss: 10376.816\n",
      "Iteration 120, Loss: 10528.935\n",
      "Iteration 130, Loss: 10148.260\n",
      "Iteration 140, Loss: 9720.283\n",
      "Iteration 150, Loss: 10570.404\n",
      "Iteration 160, Loss: 9408.493\n",
      "Iteration 170, Loss: 8972.217\n",
      "Iteration 180, Loss: 9044.120\n",
      "Iteration 190, Loss: 8548.711\n",
      "Iteration 199, Loss: 8571.260\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 15442.394\n",
      "Iteration 020, Loss: 15337.649\n",
      "Iteration 030, Loss: 15199.546\n",
      "Iteration 040, Loss: 15008.901\n",
      "Iteration 050, Loss: 14702.308\n",
      "Iteration 060, Loss: 14272.523\n",
      "Iteration 070, Loss: 13723.741\n",
      "Iteration 080, Loss: 12848.697\n",
      "Iteration 090, Loss: 12211.316\n",
      "Iteration 100, Loss: 11434.491\n",
      "Iteration 110, Loss: 10616.585\n",
      "Iteration 120, Loss: 10376.119\n",
      "Iteration 130, Loss: 9890.679\n",
      "Iteration 140, Loss: 8285.401\n",
      "Iteration 150, Loss: 7307.869\n",
      "Iteration 160, Loss: 7468.826\n",
      "Iteration 170, Loss: 7405.766\n",
      "Iteration 180, Loss: 7194.726\n",
      "Iteration 190, Loss: 7031.373\n",
      "Iteration 199, Loss: 7026.036\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 15433.478\n",
      "Iteration 020, Loss: 15312.174\n",
      "Iteration 030, Loss: 15146.046\n",
      "Iteration 040, Loss: 14892.162\n",
      "Iteration 050, Loss: 14486.588\n",
      "Iteration 060, Loss: 13918.737\n",
      "Iteration 070, Loss: 13082.454\n",
      "Iteration 080, Loss: 12282.787\n",
      "Iteration 090, Loss: 11414.294\n",
      "Iteration 100, Loss: 10547.955\n",
      "Iteration 110, Loss: 10346.645\n",
      "Iteration 120, Loss: 9599.165\n",
      "Iteration 130, Loss: 7441.332\n",
      "Iteration 140, Loss: 7391.267\n",
      "Iteration 150, Loss: 7409.385\n",
      "Iteration 160, Loss: 7203.799\n",
      "Iteration 170, Loss: 7019.630\n",
      "Iteration 180, Loss: 7000.293\n",
      "Iteration 190, Loss: 6893.473\n",
      "Iteration 199, Loss: 6932.076\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 15362.140\n",
      "Iteration 020, Loss: 15062.918\n",
      "Iteration 030, Loss: 14398.049\n",
      "Iteration 040, Loss: 13161.026\n",
      "Iteration 050, Loss: 11589.204\n",
      "Iteration 060, Loss: 10254.395\n",
      "Iteration 070, Loss: 9321.067\n",
      "Iteration 080, Loss: 7017.005\n",
      "Iteration 090, Loss: 7544.968\n",
      "Iteration 100, Loss: 7307.658\n",
      "Iteration 110, Loss: 7145.011\n",
      "Iteration 120, Loss: 7113.486\n",
      "Iteration 130, Loss: 7117.059\n",
      "Iteration 140, Loss: 7163.801\n",
      "Iteration 150, Loss: 7212.435\n",
      "Iteration 160, Loss: 7124.714\n",
      "Iteration 170, Loss: 7100.564\n",
      "Iteration 180, Loss: 7184.737\n",
      "Iteration 190, Loss: 7509.501\n",
      "Iteration 199, Loss: 7574.598\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 15025.666\n",
      "Iteration 020, Loss: 11312.243\n",
      "Iteration 030, Loss: 9115.244\n",
      "Iteration 040, Loss: 8762.512\n",
      "Iteration 050, Loss: 8574.528\n",
      "Iteration 060, Loss: 8803.755\n",
      "Iteration 070, Loss: 8651.481\n",
      "Iteration 080, Loss: 8195.229\n",
      "Iteration 090, Loss: 8331.640\n",
      "Iteration 100, Loss: 8428.754\n",
      "Iteration 110, Loss: 9257.049\n",
      "Iteration 120, Loss: 11447.635\n",
      "Iteration 130, Loss: 9850.670\n",
      "Iteration 140, Loss: 10384.117\n",
      "Iteration 150, Loss: 8746.461\n",
      "Iteration 160, Loss: 8506.183\n",
      "Iteration 170, Loss: 8519.899\n",
      "Iteration 180, Loss: 8458.357\n",
      "Iteration 190, Loss: 8130.657\n",
      "Iteration 199, Loss: 8665.445\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 15506.996\n",
      "Iteration 020, Loss: 15492.031\n",
      "Iteration 030, Loss: 15476.502\n",
      "Iteration 040, Loss: 15460.112\n",
      "Iteration 050, Loss: 15442.411\n",
      "Iteration 060, Loss: 15423.926\n",
      "Iteration 070, Loss: 15404.467\n",
      "Iteration 080, Loss: 15383.782\n",
      "Iteration 090, Loss: 15361.148\n",
      "Iteration 100, Loss: 15337.640\n",
      "Iteration 110, Loss: 15312.815\n",
      "Iteration 120, Loss: 15285.869\n",
      "Iteration 130, Loss: 15257.671\n",
      "Iteration 140, Loss: 15228.741\n",
      "Iteration 150, Loss: 15198.905\n",
      "Iteration 160, Loss: 15166.736\n",
      "Iteration 170, Loss: 15130.563\n",
      "Iteration 180, Loss: 15093.221\n",
      "Iteration 190, Loss: 15051.929\n",
      "Iteration 199, Loss: 15011.728\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 15505.515\n",
      "Iteration 020, Loss: 15488.870\n",
      "Iteration 030, Loss: 15471.274\n",
      "Iteration 040, Loss: 15452.518\n",
      "Iteration 050, Loss: 15432.603\n",
      "Iteration 060, Loss: 15411.206\n",
      "Iteration 070, Loss: 15388.790\n",
      "Iteration 080, Loss: 15363.994\n",
      "Iteration 090, Loss: 15337.894\n",
      "Iteration 100, Loss: 15310.142\n",
      "Iteration 110, Loss: 15279.965\n",
      "Iteration 120, Loss: 15248.550\n",
      "Iteration 130, Loss: 15215.927\n",
      "Iteration 140, Loss: 15182.111\n",
      "Iteration 150, Loss: 15143.245\n",
      "Iteration 160, Loss: 15102.523\n",
      "Iteration 170, Loss: 15057.438\n",
      "Iteration 180, Loss: 15007.706\n",
      "Iteration 190, Loss: 14950.776\n",
      "Iteration 199, Loss: 14893.720\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 15495.022\n",
      "Iteration 020, Loss: 15463.476\n",
      "Iteration 030, Loss: 15427.719\n",
      "Iteration 040, Loss: 15388.052\n",
      "Iteration 050, Loss: 15342.521\n",
      "Iteration 060, Loss: 15291.778\n",
      "Iteration 070, Loss: 15235.441\n",
      "Iteration 080, Loss: 15174.695\n",
      "Iteration 090, Loss: 15102.223\n",
      "Iteration 100, Loss: 15018.312\n",
      "Iteration 110, Loss: 14912.781\n",
      "Iteration 120, Loss: 14785.958\n",
      "Iteration 130, Loss: 14643.664\n",
      "Iteration 140, Loss: 14479.137\n",
      "Iteration 150, Loss: 14292.673\n",
      "Iteration 160, Loss: 14083.372\n",
      "Iteration 170, Loss: 13858.042\n",
      "Iteration 180, Loss: 13629.201\n",
      "Iteration 190, Loss: 13327.359\n",
      "Iteration 199, Loss: 12912.230\n",
      "Iteration 000, Loss: 15520.702\n",
      "Iteration 010, Loss: 15457.773\n",
      "Iteration 020, Loss: 15290.512\n",
      "Iteration 030, Loss: 14984.225\n",
      "Iteration 040, Loss: 14299.152\n",
      "Iteration 050, Loss: 13147.253\n",
      "Iteration 060, Loss: 11649.235\n",
      "Iteration 070, Loss: 10389.437\n",
      "Iteration 080, Loss: 9795.212\n",
      "Iteration 090, Loss: 9186.207\n",
      "Iteration 100, Loss: 7770.934\n",
      "Iteration 110, Loss: 6892.134\n",
      "Iteration 120, Loss: 6137.475\n",
      "Iteration 130, Loss: 5992.246\n",
      "Iteration 140, Loss: 6927.580\n",
      "Iteration 150, Loss: 7605.582\n",
      "Iteration 160, Loss: 7469.269\n",
      "Iteration 170, Loss: 7382.197\n",
      "Iteration 180, Loss: 7307.371\n",
      "Iteration 190, Loss: 7200.910\n",
      "Iteration 199, Loss: 7465.166\n"
     ]
    }
   ],
   "source": [
    "optimized32 = {}\n",
    "for p in params:\n",
    "    s = optimize('1z6mA02', random_state=2, iterations=200, lr=p[0], \n",
    "                 momentum=p[1], normal=True, angle_potential=False, normalize_gradients=False,\n",
    "                scale_gradients=False, normalize_gradients_2=True)\n",
    "    optimized32[f'{p}'] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('1z6mA02_trials/1z6m_smooth_standdev_32.pkl', 'wb') as f:\n",
    "    pickle.dump(optimized32, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 7823.742\n",
      "Iteration 020, Loss: 8440.073\n",
      "Iteration 030, Loss: 7475.068\n",
      "Iteration 040, Loss: 7540.645\n",
      "Iteration 050, Loss: 7745.795\n",
      "Iteration 060, Loss: 7958.589\n",
      "Iteration 070, Loss: 8532.984\n",
      "Iteration 080, Loss: 8125.640\n",
      "Iteration 090, Loss: 8853.299\n",
      "Iteration 100, Loss: 8737.996\n",
      "Iteration 110, Loss: 8040.242\n",
      "Iteration 120, Loss: 8181.139\n",
      "Iteration 130, Loss: 7642.226\n",
      "Iteration 140, Loss: 8613.634\n",
      "Iteration 150, Loss: 8848.365\n",
      "Iteration 160, Loss: 9078.021\n",
      "Iteration 170, Loss: 8133.326\n",
      "Iteration 180, Loss: 8206.358\n",
      "Iteration 190, Loss: 7858.870\n",
      "Iteration 199, Loss: 8202.895\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 7731.225\n",
      "Iteration 020, Loss: 8634.027\n",
      "Iteration 030, Loss: 8466.965\n",
      "Iteration 040, Loss: 9191.010\n",
      "Iteration 050, Loss: 8916.810\n",
      "Iteration 060, Loss: 8499.615\n",
      "Iteration 070, Loss: 8587.984\n",
      "Iteration 080, Loss: 8173.245\n",
      "Iteration 090, Loss: 8658.265\n",
      "Iteration 100, Loss: 9102.695\n",
      "Iteration 110, Loss: 9175.891\n",
      "Iteration 120, Loss: 8771.303\n",
      "Iteration 130, Loss: 9061.896\n",
      "Iteration 140, Loss: 8898.093\n",
      "Iteration 150, Loss: 8269.363\n",
      "Iteration 160, Loss: 8044.270\n",
      "Iteration 170, Loss: 7963.429\n",
      "Iteration 180, Loss: 10141.635\n",
      "Iteration 190, Loss: 8969.611\n",
      "Iteration 199, Loss: 8672.225\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 9063.020\n",
      "Iteration 020, Loss: 8929.216\n",
      "Iteration 030, Loss: 8724.421\n",
      "Iteration 040, Loss: 10906.635\n",
      "Iteration 050, Loss: 8534.881\n",
      "Iteration 060, Loss: 8413.856\n",
      "Iteration 070, Loss: 9567.104\n",
      "Iteration 080, Loss: 9117.981\n",
      "Iteration 090, Loss: 9323.723\n",
      "Iteration 100, Loss: 8917.922\n",
      "Iteration 110, Loss: 9764.769\n",
      "Iteration 120, Loss: 8997.139\n",
      "Iteration 130, Loss: 8984.865\n",
      "Iteration 140, Loss: 8214.952\n",
      "Iteration 150, Loss: 9384.058\n",
      "Iteration 160, Loss: 9044.857\n",
      "Iteration 170, Loss: 8685.121\n",
      "Iteration 180, Loss: 10042.045\n",
      "Iteration 190, Loss: 9374.700\n",
      "Iteration 199, Loss: 8357.810\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 10657.344\n",
      "Iteration 020, Loss: 9406.131\n",
      "Iteration 030, Loss: 10864.572\n",
      "Iteration 040, Loss: 10883.083\n",
      "Iteration 050, Loss: 9195.937\n",
      "Iteration 060, Loss: 11157.413\n",
      "Iteration 070, Loss: 9478.077\n",
      "Iteration 080, Loss: 10933.072\n",
      "Iteration 090, Loss: 12182.696\n",
      "Iteration 100, Loss: 11569.758\n",
      "Iteration 110, Loss: 12255.499\n",
      "Iteration 120, Loss: 10289.714\n",
      "Iteration 130, Loss: 11427.673\n",
      "Iteration 140, Loss: 11963.215\n",
      "Iteration 150, Loss: 10731.583\n",
      "Iteration 160, Loss: 10179.561\n",
      "Iteration 170, Loss: 12110.731\n",
      "Iteration 180, Loss: 11115.469\n",
      "Iteration 190, Loss: 10520.381\n",
      "Iteration 199, Loss: 10009.434\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 9190.028\n",
      "Iteration 020, Loss: 9029.492\n",
      "Iteration 030, Loss: 9025.981\n",
      "Iteration 040, Loss: 8895.079\n",
      "Iteration 050, Loss: 8594.669\n",
      "Iteration 060, Loss: 8518.022\n",
      "Iteration 070, Loss: 8282.016\n",
      "Iteration 080, Loss: 8994.690\n",
      "Iteration 090, Loss: 8557.268\n",
      "Iteration 100, Loss: 8394.331\n",
      "Iteration 110, Loss: 8559.100\n",
      "Iteration 120, Loss: 8093.294\n",
      "Iteration 130, Loss: 8539.007\n",
      "Iteration 140, Loss: 8401.948\n",
      "Iteration 150, Loss: 8855.227\n",
      "Iteration 160, Loss: 8631.657\n",
      "Iteration 170, Loss: 8111.321\n",
      "Iteration 180, Loss: 8802.974\n",
      "Iteration 190, Loss: 7843.816\n",
      "Iteration 199, Loss: 8377.273\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 8275.672\n",
      "Iteration 020, Loss: 9458.380\n",
      "Iteration 030, Loss: 7940.708\n",
      "Iteration 040, Loss: 7492.619\n",
      "Iteration 050, Loss: 7627.617\n",
      "Iteration 060, Loss: 8062.228\n",
      "Iteration 070, Loss: 7607.608\n",
      "Iteration 080, Loss: 7518.036\n",
      "Iteration 090, Loss: 8569.950\n",
      "Iteration 100, Loss: 8029.532\n",
      "Iteration 110, Loss: 7746.345\n",
      "Iteration 120, Loss: 7941.723\n",
      "Iteration 130, Loss: 8149.931\n",
      "Iteration 140, Loss: 8546.394\n",
      "Iteration 150, Loss: 7875.368\n",
      "Iteration 160, Loss: 7811.901\n",
      "Iteration 170, Loss: 8100.317\n",
      "Iteration 180, Loss: 7847.147\n",
      "Iteration 190, Loss: 7930.438\n",
      "Iteration 199, Loss: 7930.657\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 8005.599\n",
      "Iteration 020, Loss: 9053.975\n",
      "Iteration 030, Loss: 8519.583\n",
      "Iteration 040, Loss: 8131.310\n",
      "Iteration 050, Loss: 8458.199\n",
      "Iteration 060, Loss: 9323.700\n",
      "Iteration 070, Loss: 8964.721\n",
      "Iteration 080, Loss: 8027.591\n",
      "Iteration 090, Loss: 8647.917\n",
      "Iteration 100, Loss: 8496.070\n",
      "Iteration 110, Loss: 8764.511\n",
      "Iteration 120, Loss: 8891.679\n",
      "Iteration 130, Loss: 8166.402\n",
      "Iteration 140, Loss: 8074.874\n",
      "Iteration 150, Loss: 8533.886\n",
      "Iteration 160, Loss: 7969.684\n",
      "Iteration 170, Loss: 8296.736\n",
      "Iteration 180, Loss: 8437.640\n",
      "Iteration 190, Loss: 7584.482\n",
      "Iteration 199, Loss: 7467.294\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 9676.603\n",
      "Iteration 020, Loss: 9874.437\n",
      "Iteration 030, Loss: 11354.649\n",
      "Iteration 040, Loss: 11424.177\n",
      "Iteration 050, Loss: 11542.849\n",
      "Iteration 060, Loss: 11380.881\n",
      "Iteration 070, Loss: 10832.001\n",
      "Iteration 080, Loss: 9910.555\n",
      "Iteration 090, Loss: 10256.015\n",
      "Iteration 100, Loss: 9940.388\n",
      "Iteration 110, Loss: 12030.253\n",
      "Iteration 120, Loss: 11108.086\n",
      "Iteration 130, Loss: 10160.405\n",
      "Iteration 140, Loss: 10299.689\n",
      "Iteration 150, Loss: 9616.021\n",
      "Iteration 160, Loss: 9277.635\n",
      "Iteration 170, Loss: 11143.316\n",
      "Iteration 180, Loss: 10778.477\n",
      "Iteration 190, Loss: 9003.033\n",
      "Iteration 199, Loss: 9759.256\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 8098.995\n",
      "Iteration 020, Loss: 8015.408\n",
      "Iteration 030, Loss: 7558.999\n",
      "Iteration 040, Loss: 8525.818\n",
      "Iteration 050, Loss: 8953.617\n",
      "Iteration 060, Loss: 8319.391\n",
      "Iteration 070, Loss: 7986.462\n",
      "Iteration 080, Loss: 7814.217\n",
      "Iteration 090, Loss: 7738.676\n",
      "Iteration 100, Loss: 7764.866\n",
      "Iteration 110, Loss: 7803.223\n",
      "Iteration 120, Loss: 8190.373\n",
      "Iteration 130, Loss: 7734.714\n",
      "Iteration 140, Loss: 7751.071\n",
      "Iteration 150, Loss: 7769.850\n",
      "Iteration 160, Loss: 7907.949\n",
      "Iteration 170, Loss: 7488.428\n",
      "Iteration 180, Loss: 6856.900\n",
      "Iteration 190, Loss: 7444.600\n",
      "Iteration 199, Loss: 7569.338\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 8066.063\n",
      "Iteration 020, Loss: 7843.589\n",
      "Iteration 030, Loss: 7635.306\n",
      "Iteration 040, Loss: 8752.571\n",
      "Iteration 050, Loss: 8750.588\n",
      "Iteration 060, Loss: 8107.301\n",
      "Iteration 070, Loss: 7867.945\n",
      "Iteration 080, Loss: 7758.436\n",
      "Iteration 090, Loss: 7736.933\n",
      "Iteration 100, Loss: 7748.076\n",
      "Iteration 110, Loss: 7279.150\n",
      "Iteration 120, Loss: 7840.215\n",
      "Iteration 130, Loss: 7975.897\n",
      "Iteration 140, Loss: 7606.291\n",
      "Iteration 150, Loss: 6723.999\n",
      "Iteration 160, Loss: 7400.499\n",
      "Iteration 170, Loss: 7913.088\n",
      "Iteration 180, Loss: 8051.142\n",
      "Iteration 190, Loss: 7865.490\n",
      "Iteration 199, Loss: 7897.971\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 8365.058\n",
      "Iteration 020, Loss: 7592.066\n",
      "Iteration 030, Loss: 9337.861\n",
      "Iteration 040, Loss: 8195.522\n",
      "Iteration 050, Loss: 8082.876\n",
      "Iteration 060, Loss: 7761.174\n",
      "Iteration 070, Loss: 8026.131\n",
      "Iteration 080, Loss: 7938.938\n",
      "Iteration 090, Loss: 7875.915\n",
      "Iteration 100, Loss: 7662.614\n",
      "Iteration 110, Loss: 7792.222\n",
      "Iteration 120, Loss: 8177.368\n",
      "Iteration 130, Loss: 8524.912\n",
      "Iteration 140, Loss: 7963.757\n",
      "Iteration 150, Loss: 7631.164\n",
      "Iteration 160, Loss: 7717.660\n",
      "Iteration 170, Loss: 8185.187\n",
      "Iteration 180, Loss: 8267.318\n",
      "Iteration 190, Loss: 8082.765\n",
      "Iteration 199, Loss: 8608.960\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 8426.982\n",
      "Iteration 020, Loss: 9130.028\n",
      "Iteration 030, Loss: 8762.059\n",
      "Iteration 040, Loss: 8277.753\n",
      "Iteration 050, Loss: 9883.945\n",
      "Iteration 060, Loss: 10031.956\n",
      "Iteration 070, Loss: 10318.132\n",
      "Iteration 080, Loss: 9089.222\n",
      "Iteration 090, Loss: 11205.836\n",
      "Iteration 100, Loss: 9635.253\n",
      "Iteration 110, Loss: 9617.092\n",
      "Iteration 120, Loss: 9244.572\n",
      "Iteration 130, Loss: 10027.557\n",
      "Iteration 140, Loss: 9599.742\n",
      "Iteration 150, Loss: 10110.724\n",
      "Iteration 160, Loss: 9204.356\n",
      "Iteration 170, Loss: 9639.144\n",
      "Iteration 180, Loss: 9272.076\n",
      "Iteration 190, Loss: 8984.698\n",
      "Iteration 199, Loss: 9463.239\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 8441.448\n",
      "Iteration 020, Loss: 8071.408\n",
      "Iteration 030, Loss: 8360.663\n",
      "Iteration 040, Loss: 7966.392\n",
      "Iteration 050, Loss: 7578.379\n",
      "Iteration 060, Loss: 7571.934\n",
      "Iteration 070, Loss: 7898.004\n",
      "Iteration 080, Loss: 8503.419\n",
      "Iteration 090, Loss: 8725.551\n",
      "Iteration 100, Loss: 8714.184\n",
      "Iteration 110, Loss: 8466.504\n",
      "Iteration 120, Loss: 8236.527\n",
      "Iteration 130, Loss: 7995.094\n",
      "Iteration 140, Loss: 7822.813\n",
      "Iteration 150, Loss: 7683.172\n",
      "Iteration 160, Loss: 7699.682\n",
      "Iteration 170, Loss: 7760.954\n",
      "Iteration 180, Loss: 7710.260\n",
      "Iteration 190, Loss: 7732.922\n",
      "Iteration 199, Loss: 7742.026\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 8367.770\n",
      "Iteration 020, Loss: 8148.860\n",
      "Iteration 030, Loss: 8352.524\n",
      "Iteration 040, Loss: 7779.297\n",
      "Iteration 050, Loss: 7448.551\n",
      "Iteration 060, Loss: 7746.121\n",
      "Iteration 070, Loss: 8327.575\n",
      "Iteration 080, Loss: 8706.838\n",
      "Iteration 090, Loss: 8760.244\n",
      "Iteration 100, Loss: 8500.646\n",
      "Iteration 110, Loss: 8226.107\n",
      "Iteration 120, Loss: 7971.980\n",
      "Iteration 130, Loss: 7811.044\n",
      "Iteration 140, Loss: 7745.703\n",
      "Iteration 150, Loss: 7835.292\n",
      "Iteration 160, Loss: 7845.575\n",
      "Iteration 170, Loss: 7760.482\n",
      "Iteration 180, Loss: 7742.595\n",
      "Iteration 190, Loss: 7832.955\n",
      "Iteration 199, Loss: 8090.734\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 8050.544\n",
      "Iteration 020, Loss: 8302.134\n",
      "Iteration 030, Loss: 7594.352\n",
      "Iteration 040, Loss: 7756.026\n",
      "Iteration 050, Loss: 8895.157\n",
      "Iteration 060, Loss: 8921.521\n",
      "Iteration 070, Loss: 8204.511\n",
      "Iteration 080, Loss: 8019.747\n",
      "Iteration 090, Loss: 8019.312\n",
      "Iteration 100, Loss: 7847.040\n",
      "Iteration 110, Loss: 7842.316\n",
      "Iteration 120, Loss: 8040.354\n",
      "Iteration 130, Loss: 8069.849\n",
      "Iteration 140, Loss: 8014.039\n",
      "Iteration 150, Loss: 7806.773\n",
      "Iteration 160, Loss: 7610.083\n",
      "Iteration 170, Loss: 8304.014\n",
      "Iteration 180, Loss: 8680.059\n",
      "Iteration 190, Loss: 8626.229\n",
      "Iteration 199, Loss: 8544.798\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 8775.117\n",
      "Iteration 020, Loss: 9309.365\n",
      "Iteration 030, Loss: 8935.228\n",
      "Iteration 040, Loss: 9233.950\n",
      "Iteration 050, Loss: 9491.338\n",
      "Iteration 060, Loss: 8556.951\n",
      "Iteration 070, Loss: 9072.202\n",
      "Iteration 080, Loss: 8472.220\n",
      "Iteration 090, Loss: 8366.501\n",
      "Iteration 100, Loss: 8792.999\n",
      "Iteration 110, Loss: 8835.595\n",
      "Iteration 120, Loss: 8878.552\n",
      "Iteration 130, Loss: 8786.929\n",
      "Iteration 140, Loss: 8690.624\n",
      "Iteration 150, Loss: 8628.331\n",
      "Iteration 160, Loss: 9940.632\n",
      "Iteration 170, Loss: 9079.750\n",
      "Iteration 180, Loss: 9574.338\n",
      "Iteration 190, Loss: 8913.947\n",
      "Iteration 199, Loss: 9226.108\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 8943.520\n",
      "Iteration 020, Loss: 8836.202\n",
      "Iteration 030, Loss: 8706.847\n",
      "Iteration 040, Loss: 8577.173\n",
      "Iteration 050, Loss: 8432.862\n",
      "Iteration 060, Loss: 8302.905\n",
      "Iteration 070, Loss: 8180.399\n",
      "Iteration 080, Loss: 8093.726\n",
      "Iteration 090, Loss: 8070.707\n",
      "Iteration 100, Loss: 8104.901\n",
      "Iteration 110, Loss: 8157.756\n",
      "Iteration 120, Loss: 8226.179\n",
      "Iteration 130, Loss: 8291.222\n",
      "Iteration 140, Loss: 8347.311\n",
      "Iteration 150, Loss: 8373.811\n",
      "Iteration 160, Loss: 8366.049\n",
      "Iteration 170, Loss: 8291.650\n",
      "Iteration 180, Loss: 8155.946\n",
      "Iteration 190, Loss: 8031.591\n",
      "Iteration 199, Loss: 7937.774\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 8933.895\n",
      "Iteration 020, Loss: 8811.768\n",
      "Iteration 030, Loss: 8668.271\n",
      "Iteration 040, Loss: 8514.134\n",
      "Iteration 050, Loss: 8360.277\n",
      "Iteration 060, Loss: 8221.316\n",
      "Iteration 070, Loss: 8103.861\n",
      "Iteration 080, Loss: 8071.899\n",
      "Iteration 090, Loss: 8101.020\n",
      "Iteration 100, Loss: 8160.367\n",
      "Iteration 110, Loss: 8238.356\n",
      "Iteration 120, Loss: 8308.146\n",
      "Iteration 130, Loss: 8363.383\n",
      "Iteration 140, Loss: 8375.364\n",
      "Iteration 150, Loss: 8332.741\n",
      "Iteration 160, Loss: 8194.159\n",
      "Iteration 170, Loss: 8050.894\n",
      "Iteration 180, Loss: 7932.799\n",
      "Iteration 190, Loss: 7840.034\n",
      "Iteration 199, Loss: 7761.511\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 8857.920\n",
      "Iteration 020, Loss: 8608.850\n",
      "Iteration 030, Loss: 8335.143\n",
      "Iteration 040, Loss: 8108.629\n",
      "Iteration 050, Loss: 8067.535\n",
      "Iteration 060, Loss: 8199.172\n",
      "Iteration 070, Loss: 8313.213\n",
      "Iteration 080, Loss: 8372.564\n",
      "Iteration 090, Loss: 8242.913\n",
      "Iteration 100, Loss: 7982.750\n",
      "Iteration 110, Loss: 7807.332\n",
      "Iteration 120, Loss: 7657.760\n",
      "Iteration 130, Loss: 7516.569\n",
      "Iteration 140, Loss: 7461.250\n",
      "Iteration 150, Loss: 7531.120\n",
      "Iteration 160, Loss: 7656.257\n",
      "Iteration 170, Loss: 7782.161\n",
      "Iteration 180, Loss: 7944.728\n",
      "Iteration 190, Loss: 8215.607\n",
      "Iteration 199, Loss: 8424.476\n",
      "Iteration 000, Loss: 9047.678\n",
      "Iteration 010, Loss: 8588.799\n",
      "Iteration 020, Loss: 8064.471\n",
      "Iteration 030, Loss: 8484.999\n",
      "Iteration 040, Loss: 8041.223\n",
      "Iteration 050, Loss: 7675.813\n",
      "Iteration 060, Loss: 8266.315\n",
      "Iteration 070, Loss: 8264.028\n",
      "Iteration 080, Loss: 8607.323\n",
      "Iteration 090, Loss: 8655.272\n",
      "Iteration 100, Loss: 8618.047\n",
      "Iteration 110, Loss: 8820.178\n",
      "Iteration 120, Loss: 9270.952\n",
      "Iteration 130, Loss: 9255.229\n",
      "Iteration 140, Loss: 8815.683\n",
      "Iteration 150, Loss: 8128.990\n",
      "Iteration 160, Loss: 7791.394\n",
      "Iteration 170, Loss: 7753.863\n",
      "Iteration 180, Loss: 7851.618\n",
      "Iteration 190, Loss: 7603.224\n",
      "Iteration 199, Loss: 7461.782\n"
     ]
    }
   ],
   "source": [
    "optimized33 = {}\n",
    "for p in params:\n",
    "    s = optimize('1z6mA02', random_state=3, iterations=200, lr=p[0], \n",
    "                 momentum=p[1], normal=True, angle_potential=False, normalize_gradients=False,\n",
    "                scale_gradients=False, normalize_gradients_2=True)\n",
    "    optimized33[f'{p}'] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('1z6mA02_trials/1z6m_smooth_standdev_33.pkl', 'wb') as f:\n",
    "    pickle.dump(optimized33, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
